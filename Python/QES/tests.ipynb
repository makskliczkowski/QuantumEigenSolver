{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory (project root) to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "This set of tests for model validation and performance evaluation. It includes creation of models from operators and from the specialized model classes. It also includes the evaluation of the models on a set of test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Any Hamiltonian defined from the operators\n",
    "\n",
    "Underneath, the Hamiltonian is constructed using the defined operators, such as `sig_x` and `sig_z`, which act on specific sites or globally on the lattice. These operators are combined with appropriate multipliers to represent the physical interactions in the system. The Hamiltonian is then built, diagonalized, and used to compute eigenvalues, eigenvectors, and other properties of the quantum system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Algebra.hamil as hamil\n",
    "from Algebra.Operator.operator import  test_operators\n",
    "from general_python.lattices.honeycomb import HoneycombLattice\n",
    "from general_python.lattices.lattice import LatticeBC\n",
    "\n",
    "backend = 'np'\n",
    "lat     = HoneycombLattice( dim     = 2,\n",
    "                            lx      = 2,\n",
    "                            ly      = 2,\n",
    "                            lz      = 1,\n",
    "                            bc      = LatticeBC.PBC)\n",
    "ham     = hamil.Hamiltonian(\n",
    "    hilbert_space   = None,\n",
    "    is_sparse       = True,\n",
    "    dtype           = np.float64,\n",
    "    lattice         = lat,\n",
    "    backend         = backend,\n",
    ")\n",
    "print(lat)\n",
    "print()\n",
    "# create a set of states and test it\n",
    "(int_state, np_state, jnp_state), operators = hamil.test_generic_hamiltonian(ham=ham, ns=lat.ns)\n",
    "sig_x, sig_z, sig_z_0, sig_x_2, sig_z_sig_z, sig_z_loc, sig_x_loc, sig_z_cor = operators\n",
    "print(f\"int_state: {int_state}\")\n",
    "print(f\"np_state: {np_state}\")\n",
    "print(f\"jnp_state: {jnp_state}\")\n",
    "\n",
    "# reset the Hamiltonian operators\n",
    "ham.reset_operators()\n",
    "\n",
    "# add operators to the Hamiltonian\n",
    "ham.add(\n",
    "    operator    = sig_x,\n",
    "    multiplier  = 1.0,\n",
    "    sites       = None,\n",
    "    # modifies    = True\n",
    ")\n",
    "\n",
    "ham.add(\n",
    "    operator    = sig_z,\n",
    "    multiplier  = 0.5,\n",
    "    sites       = None,\n",
    "    # modifies    = False\n",
    ")\n",
    "\n",
    "# add some local s_z\n",
    "ham.add(\n",
    "    operator    = sig_z_loc,\n",
    "    multiplier  = -5.0,\n",
    "    sites       = [5],\n",
    "    # modifies    = False\n",
    ")\n",
    "\n",
    "# add some local s_x\n",
    "ham.add(\n",
    "    operator    = sig_x_loc,\n",
    "    multiplier  = -5.0,\n",
    "    sites       = [4],\n",
    "    # modifies    = True\n",
    ")\n",
    "\n",
    "ham._set_local_energy_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\sigma _x^0 \\sigma_x^2 |\\psi ^{\\rm int}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_operators(sig_x, int_state, ns = lat.ns, output_format='tabs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\sigma _z^0 \\sigma_z^2 |\\psi ^{\\rm int}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_operators(sig_z, int_state, ns = lat.ns, output_format='tabs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $2 * \\sigma _x^0 \\sigma_x^2 |\\psi ^{\\rm int}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_operators(sig_x_2, int_state, ns = lat.ns, output_format='tabs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\sigma _z (i) |\\psi ^{\\rm int}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_operators(sig_z_loc, int_state, ns = lat.ns, output_format='tabs', add_args=(5,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\sigma _z^0  (\\sigma _z^0 \\sigma_x^2) |\\psi ^{\\rm int}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit -r 5 -n 20 sig_z_sig_z(int_state)\n",
    "sig_z_sig_z(int_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $H |\\psi ^{\\rm int} \\rangle $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 5 -n 20\n",
    "for i in range(lat.ns):\n",
    "    ham.loc_energy(k = int_state, i = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(lat.ns):\n",
    "    print(ham.loc_energy(k = int_state, i = i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\sigma _x^0 \\sigma_x^2 |\\psi ^{\\rm np}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_operators(sig_x, np_state, ns = lat.ns, output_format='tabs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\sigma _z^0 \\sigma_z^2 |\\psi ^{\\rm np}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_operators(sig_z, np_state, ns = lat.ns, output_format='tabs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $2 * \\sigma _x^0 \\sigma_x^2 |\\psi ^{\\rm np}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_operators(sig_x_2, np_state, ns = lat.ns, output_format='tabs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $H |\\psi ^{\\rm np} \\rangle $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 5 -n 5 ham.loc_energy(k = np_state)\n",
    "ham.loc_energy(k = np_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\sigma _x^0 \\sigma_x^2 |\\psi ^{\\rm jax}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_operators(sig_x, jnp_state, ns = lat.ns, output_format='tabs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\sigma _z^0 \\sigma_z^2 |\\psi ^{\\rm jax}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_operators(sig_z, jnp_state, ns = lat.ns, output_format='tabs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $2 * \\sigma _x^0 \\sigma_x^2 |\\psi ^{\\rm jax}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_operators(sig_x_2, jnp_state, ns = lat.ns, output_format='tabs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $H |\\psi ^{\\rm jax} \\rangle $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 5 -n 5 result = ham.loc_energy(k=jnp_state)\n",
    "ham.loc_energy(k=jnp_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham.build(verbose=True, use_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham.diagonalize(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ham.eig_val)\n",
    "ham.eig_val, ham.eig_vec, ham.eig_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Heisenberg - Kitaev model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Algebra.Model.Interacting.Spin.heisenberg_kitaev import HeisenbergKitaev, HEI_KIT_Z_BOND_NEI, HEI_KIT_X_BOND_NEI, HEI_KIT_Y_BOND_NEI\n",
    "from general_python.lattices.honeycomb import HoneycombLattice\n",
    "from general_python.lattices.lattice import LatticeBC\n",
    "\n",
    "backend = 'np'\n",
    "lat     = HoneycombLattice( dim     = 2,\n",
    "                            lx      = 2,\n",
    "                            ly      = 2,\n",
    "                            lz      = 1,\n",
    "                            bc      = LatticeBC.PBC)\n",
    "lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printout the neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(lat.ns):\n",
    "#     print(f\"site {i}:\")\n",
    "#     n_nei = lat.get_nn_forward_num(i)\n",
    "#     print(f\"\\tn_nei:{n_nei}\")\n",
    "    \n",
    "#     for j in range(n_nei):\n",
    "#         nei = lat.get_nn_forward(i, j)\n",
    "#         print(f\"\\t\\tnei {j}:\\t{nei}\", end='')\n",
    "#         if j == HEI_KIT_Z_BOND_NEI:\n",
    "#             print(f\"\\t\\t\\tbond Z\")\n",
    "#         elif j == HEI_KIT_Y_BOND_NEI:\n",
    "#             print(f\"\\t\\t\\tbond Y\")\n",
    "#         elif j == HEI_KIT_X_BOND_NEI:\n",
    "#             print(f\"\\t\\t\\tbond X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamil.build(verbose=True, use_numpy=True)\n",
    "print(f\"Memory:{hamil.memory_gb:.3e}gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamil.diagonalize(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hamil.eig_val)\n",
    "hamil.eig_val, hamil.eig_vec, hamil.eig_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# make dense\n",
    "qsm_dense   = hamil.hamil.todense() if hamil.sparse else hamil.hamil\n",
    "minimum     = qsm_dense.min()\n",
    "maximum     = qsm_dense.max()\n",
    "qsm_dense   = np.abs(qsm_dense) / np.abs(qsm_dense).max()\n",
    "\n",
    "print(\"Minimum value:\", minimum)\n",
    "print(\"Maximum value:\", maximum)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(qsm_dense, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Density Representation\")\n",
    "plt.show()\n",
    "hamil.hamil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) UM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Algebra.Model.Interacting.Spin.ultrametric import UltrametricModel\n",
    "import numpy as np\n",
    "backend = 'np'\n",
    "hamil   = UltrametricModel(ns=10, hilbert_space=None, n=2, J=1.0, seed=124,\n",
    "                        alphas=0.8, backend=backend, dtype='float64')\n",
    "hamil\n",
    "\n",
    "sum_alpha = np.sum([a**(k + 1) for k, a in enumerate(hamil.alphas)])\n",
    "print(f\"Sum of alphas: {sum_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "energies    = []\n",
    "diff_min    = []\n",
    "diff_max    = []\n",
    "nrealiz     = 3\n",
    "a           = hamil.alphas[0]\n",
    "lminusn     = hamil.ns - hamil.n\n",
    "sigma_ansatz= lambda j, a, l: (1 + j * (a**2 * (1 - a ** (2*(l)))) / (1 - a**2)) if a < 1.0 else l + 1\n",
    "for i in range(nrealiz):\n",
    "    hamil.build(verbose=False, use_numpy=True)\n",
    "    hamil.diagonalize(verbose=False)\n",
    "    energies.append(hamil.eig_val)\n",
    "    print(f\"Realization {i+1}/{nrealiz}: \")\n",
    "    print(f\"\\t\\tE_min= {hamil.eig_val[0]:.3e}, E_max={hamil.eig_val[-1]:.3e}\")\n",
    "    print(f\"\\t\\tSigma: {np.var(hamil.eig_val):.3e}\")\n",
    "    print(f\"\\t\\tMean_energy: {np.mean(hamil.eig_val):.3e}\")\n",
    "    print(f\"\\t\\tDelta E = {hamil.eig_val[-1] - hamil.eig_val[0]:.3e}\")\n",
    "    \n",
    "sigma_ansatz_calc = sigma_ansatz(hamil.J, a, lminusn)\n",
    "print(f\"\\tSigma ansatz: {sigma_ansatz_calc:.3e}\")\n",
    "bound_extremum = np.sqrt(2) * hamil.J * (1 - a**(lminusn)) / (1 - a)\n",
    "print(f\"\\tEmax edge: {bound_extremum:.3e}\")\n",
    "print(f\"\\tTwice the sigma ansatz: {np.sqrt(2) * np.sqrt(1 / (1 - a**2)):.3e}\")\n",
    "print(f\"Memory:{hamil.memory_gb:.3e}gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(diff_min, label='min')\n",
    "plt.plot(diff_max, label='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(energies)):\n",
    "    plt.plot(energies[i], label=f\"Run {i+1}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.imshow(np.abs(hamil.hamil), cmap='gray_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Python.QES.Algebra.Hamil.hamil_jit_methods as hjit\n",
    "hamiltonian = hamil.hamil\n",
    "hjit.energy_width(hamiltonian)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the bandwidths prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_python.algebra.ran_wrapper import RMT, goe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Ns          = [10, 20, 50, 100, 200]\n",
    "nrealiz     = 100\n",
    "max_dist    = {n: [] for n in Ns}\n",
    "\n",
    "for N in Ns:\n",
    "    for i in range(nrealiz):\n",
    "        rmt = goe(shape=(N, N)) / np.sqrt(N)\n",
    "        eig, _ = np.linalg.eigh(rmt)\n",
    "        max_dist[N].append(np.max(np.abs(eig - np.mean(eig))))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for N in Ns:\n",
    "    y       = np.array(max_dist[N])\n",
    "    x       = len(y) * [N]\n",
    "    color   = np.random.rand(3,)\n",
    "    plt.plot(x, y, label=f\"N={N}\", color=color, marker='o', markersize=5, linestyle='None')\n",
    "    plt.axhline(y=np.mean(y), color=color, linestyle='--', label=f\"Mean N={N}\")\n",
    "plt.xlabel(\"Realization\")\n",
    "plt.ylabel(\"Max distance from mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Python.QES.Algebra.Hamil.hamil_jit_methods as hjit\n",
    "\n",
    "N       = 1\n",
    "# nss     = np.array([5, 6, 7, 8, 9, 10, 11, 12, 13])\n",
    "# nrealiz = [200, 150, 120, 100, 70, 50, 20, 10, 2]\n",
    "nss     = np.array([5, 6, 7, 8, 9, 10, 11])\n",
    "# nss     = np.array([5, 6, 7, 8])\n",
    "nrealiz = [20, 20, 20, 10, 5, 3, 1, 1]\n",
    "alphas  = np.array([0.9, 0.3])\n",
    "alphas  = np.array([0.96, 0.86, 0.71, 0.5, 0.2])\n",
    "widths  = {(alpha, ns) : [] for ns in nss for alpha in alphas}\n",
    "bwidths = {(alpha, ns) : [] for ns in nss for alpha in alphas}\n",
    "mlvls   = {(alpha, ns) : [] for ns in nss for alpha in alphas}\n",
    "gaps    = {(alpha, ns) : [] for ns in nss for alpha in alphas}\n",
    "maxes   = {(alpha, ns) : [] for ns in nss for alpha in alphas}\n",
    "mins    = {(alpha, ns) : [] for ns in nss for alpha in alphas}\n",
    "for ins, ns in enumerate(nss):\n",
    "    nh      = 2**ns\n",
    "    for alpha in alphas:\n",
    "        hamil   = UltrametricModel(ns=ns, hilbert_space=None, n=N, J=1,\n",
    "                            alphas=alpha, backend=backend, dtype='float64')\n",
    "        for i in range(nrealiz[ins]):\n",
    "            # print(\"Realization:\", i, \"for alpha:\", alpha, \"ns:\", ns)\n",
    "            \n",
    "            # randomize the Hamiltonian\n",
    "            if i == 0:\n",
    "                # build and diagonalize the Hamiltonian\n",
    "                hamil.build(verbose=False, use_numpy=True)\n",
    "                hamil.diagonalize(verbose=False)\n",
    "            else:\n",
    "                hamil.randomize()\n",
    "                hamil.diagonalize(verbose=False)\n",
    "            # calculate the energy width\n",
    "            energy_width = hjit.energy_width(hamil.hamil) / nh\n",
    "            mean_lvl_sp  = hjit.mean_level_spacing(hamil.eig_val)\n",
    "            gap_ratio    = hjit.gap_ratio(hamil.eig_val)\n",
    "            bandwidth    = hamil.eig_val[-1] - hamil.eig_val[0]\n",
    "            widths[(alpha, ns)].append(energy_width)\n",
    "            mlvls[(alpha, ns)].append(mean_lvl_sp)\n",
    "            gaps[(alpha, ns)].append(gap_ratio)\n",
    "            bwidths[(alpha, ns)].append(bandwidth)\n",
    "            maxes[(alpha, ns)].append(hamil.eig_val[-1])\n",
    "            mins[(alpha, ns)].append(hamil.eig_val[0])\n",
    "        # print(f\"Mean energy width for ns={ns}: {np.mean(widths[(alpha, ns)]):.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.var(hamil.eig_val))\n",
    "hamil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import general_python.common.plot as plotter\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "fig, ax = plotter.Plotter.get_subplots(6, 1, 5, 20, sharex = True)\n",
    "ax[0].set_title(r'Prediction equation: $\\sigma_E = [1 + \\frac{\\alpha^2 (1 - \\alpha^{2(L)})}{1 - \\alpha^2}]$, N=' + str(N))\n",
    "prediction_min = lambda j, a, l: -2 * (1 + j * (a * (1-a**l))/ (1 - a))\n",
    "prediction_max = lambda j, a, l: 2 * (1 + j * (a * (1-a**l))/ (1 - a))\n",
    "nss     = np.array(nss)\n",
    "for alpha in alphas:\n",
    "    getcolor, colors, norm  = plotter.Plotter.get_colormap(np.arange(0, 1, 1e-2), 'viridis_r')\n",
    "    color                   = getcolor(alpha)\n",
    "\n",
    "    # fit the prediction to the function\n",
    "    # prediction_fit  = lambda L, c: c * (1 + (alpha**2 * (1 - alpha ** (2*(L-N)))) / (1 - alpha**2))\n",
    "    # popt, pcov      = curve_fit(prediction_fit, nss, [np.mean(widths[(alpha, ns)]) for ns in nss], p0=[1.0])\n",
    "    # predictions     = prediction_fit(nss, *popt)\n",
    "    prediction      = lambda L: (1 + (alpha**2 * (1 - alpha ** (2*(L-N)))) / (1 - alpha**2))\n",
    "    predictions     = prediction(nss)\n",
    "    y               = [np.mean(widths[(alpha, ns)]) for ns in nss]\n",
    "    ax[0].scatter(nss, y, color=color, label=f'$\\\\alpha$={alpha}')\n",
    "    ax[0].plot(nss, predictions, color=color, linestyle='--')\n",
    "    for ns in nss:\n",
    "        yin         = np.array(widths[(alpha, ns)])\n",
    "        ax[1].scatter([ns] * len(yin), yin, color=color, alpha=0.2)\n",
    "    ax[1].plot(nss, y, color=color, label=f'$\\\\alpha$={alpha}', linestyle='--')\n",
    "    \n",
    "    # mean level spacing\n",
    "    for ns in nss:\n",
    "        y           = np.array(mlvls[(alpha, ns)])\n",
    "        ax[2].scatter([ns] * len(y), y, color=color, alpha=0.5, marker = '.')\n",
    "    ax[2].plot(nss, [np.mean(mlvls[(alpha, ns)]) for ns in nss], color=color, label=f'$\\\\alpha$={alpha}', linestyle='-')\n",
    "    # gap ratio\n",
    "    for ns in nss:\n",
    "        y           = np.array(gaps[(alpha, ns)])\n",
    "        ax[3].scatter([ns] * len(y), y, color=color, alpha=0.2, marker = '.')\n",
    "    ax[3].plot(nss, [np.mean(gaps[(alpha, ns)]) for ns in nss], color=color, label=f'$\\\\alpha$={alpha}', linestyle='--', lw = 2)\n",
    "    ax[3].axhline(y=0.5359, color='k', linestyle='--', label='GOE')\n",
    "    ax[3].axhline(y=0.3862, color='k', linestyle='--', label='Poisson')\n",
    "    # bandwidth\n",
    "    for ns in nss:\n",
    "        y           = np.array(bwidths[(alpha, ns)])\n",
    "        ax[4].scatter([ns] * len(y), y, color=color, alpha=0.4)\n",
    "    # min/max\n",
    "    for ns in nss:\n",
    "        y           = np.array(maxes[(alpha, ns)])\n",
    "        ax[5].scatter([ns] * len(y), y, color=color, alpha=0.2, marker = 's')\n",
    "        y           = np.array(mins[(alpha, ns)])\n",
    "        ax[5].scatter([ns] * len(y), y, color=color, alpha=0.2, marker = 'o')\n",
    "        # plot the predictions\n",
    "    y           = max_ansatz(hamil.J, alpha, nss - N)\n",
    "    ax[5].plot(nss, y, color=color, linestyle='--', lw = 2)\n",
    "    y           = min_ansatz(hamil.J, alpha, nss - N)\n",
    "    ax[5].plot(nss, y, color=color, linestyle='--', lw = 2)\n",
    "                \n",
    "ax[0].set_ylabel(r'$\\bar \\sigma_E/\\mathcal{D}$')\n",
    "ax[1].set_ylabel(r'$\\sigma_E/\\mathcal{D}$')\n",
    "ax[2].set_ylabel(r'$\\Delta$')\n",
    "ax[3].set_ylabel(r'$r$')\n",
    "ax[4].set_xlabel(r'$L+N$')\n",
    "ax[4].set_ylabel(r'$|E_{max} - E_{min}|$')\n",
    "\n",
    "plotter.Plotter.set_legend(ax[0], loc='upper left', frameon=True)\n",
    "# ax[0].legend()\n",
    "ax[0].set_yscale('log')\n",
    "ax[2].set_yscale('log')\n",
    "plt.tight_layout()\n",
    "# plt.yscale('log')\n",
    "plt.savefig('ultrametric_energy_width_small.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Quantum Sun Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Algebra.Model.Interacting.Spin.qsm import QSM\n",
    "backend = 'np'\n",
    "hamil   = QSM(ns=6, hilbert_space=None, n=3, a=0.64, h=1.0, xi=0.2, backend=backend, dtype='float64')\n",
    "hamil.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamil.build(verbose=True, use_numpy=True)\n",
    "print(f\"Memory:{hamil.memory_gb:.3e}gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamil.diagonalize(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamil.eig_val[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_python.physics.eigenlevels import gap_ratio\n",
    "gap_ratio(hamil.eig_val, 1.0)['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.trace((hamil.hamil * hamil.hamil).todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamil.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamil.build(verbose=True, use_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamil.diagonalize(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamil.h_memory_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hamil.eig_val)\n",
    "hamil.eig_val, hamil.eig_vec, hamil.eig_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# make dense\n",
    "qsm_dense   = hamil.hamil.todense() if hamil.sparse else hamil.hamil\n",
    "minimum     = qsm_dense.min()\n",
    "maximum     = qsm_dense.max()\n",
    "qsm_dense   = np.abs(qsm_dense) / np.abs(qsm_dense).max()\n",
    "\n",
    "print(\"Minimum value:\", minimum)\n",
    "print(\"Maximum value:\", maximum)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(qsm_dense, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Density Representation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Algebra.Operator.operators_spin import sig_x, sig_z, OperatorTypeActing\n",
    "from general_python.common import binary\n",
    "import numpy as np\n",
    "\n",
    "backend     = 'np'\n",
    "ns          = 2\n",
    "nh          = 2**ns   \n",
    "sites       = [ns - 1]\n",
    "# sites       = [0]\n",
    "sig_x_op    = sig_x(ns = ns, type_act = OperatorTypeActing.Global, sites = sites)\n",
    "sig_z_op    = sig_z(ns = ns, type_act = OperatorTypeActing.Global, sites = sites)\n",
    "eye_first   = np.eye(2**(ns - 1))\n",
    "eye_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check how it works on the base states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### With states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_state_full          = 2**(ns) - 1\n",
    "int_state_half          = 2**(ns - 1) - 1\n",
    "int_state_half_size     = 2**(ns//2) - 1\n",
    "\n",
    "# binary representation\n",
    "backend_state           = binary.int2base(int_state_full, size=ns, backend=backend)\n",
    "backend_state_half      = binary.int2base(int_state_half, size=ns, backend=backend)\n",
    "backend_state_half_size = binary.int2base(int_state_half_size, size=ns//2, backend=backend)\n",
    "# string representation\n",
    "string_state            = binary.int2binstr(int_state_full, bits=ns)\n",
    "string_state_half       = binary.int2binstr(int_state_half, bits=ns)\n",
    "string_state_half_size  = binary.int2binstr(int_state_half_size, bits=ns)\n",
    "\n",
    "# act on the integer states\n",
    "for state in [int_state_full, int_state_half, int_state_half_size]:\n",
    "    print()\n",
    "    # sigma x\n",
    "    print(\"Applying sigma_x\")\n",
    "    print(f\"Acting on state {state} ({binary.int2binstr(state, bits=ns)})\")\n",
    "    resulting_state, resulting_value = sig_x_op(state)\n",
    "    print(f\"\\tResulting state: {resulting_state[0]}\")\n",
    "    print(f\"\\tResulting value: {resulting_value}\")\n",
    "    print(f\"\\tBinary representation of resulting state: {binary.int2binstr(resulting_state[0], bits=ns)}\")\n",
    "    # sigma z\n",
    "    print(\"Applying sigma_z\")\n",
    "    resulting_state, resulting_value = sig_z_op(state)\n",
    "    print(f\"Resulting state: {resulting_state[0]}\")\n",
    "    print(f\"Resulting value: {resulting_value}\")\n",
    "    print(f\"Binary representation of resulting state: {binary.int2binstr(resulting_state[0], bits=ns)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### With Hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Algebra.Model.Interacting.Spin.qsm import QSM\n",
    "from general_python.algebra.linalg import act, overlap\n",
    "from general_python.common.plot import Plotter, colorsCycle\n",
    "from general_python.maths.statistics import Statistics, HistogramAverage, Fraction\n",
    "from Algebra.Operator.operators_spin import sig_x, sig_z, OperatorTypeActing\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype           = 'float64'\n",
    "nrealizations   = (np.array([100, 100, 100, 50, 30, 20])/5).astype(int)\n",
    "# nrealizations   = (np.array([1, 1, 1, 1, 1, 1])).astype(int)\n",
    "alpha           = 0.8\n",
    "histograms_x    = {}\n",
    "histograms_z    = {}\n",
    "diagonals_x     = {}\n",
    "diagonals_z     = {}\n",
    "energies        = {}\n",
    "nss             = [6,7,8,9,10]\n",
    "# nss             = [10, 11]\n",
    "\n",
    "# nss             = [9,10,11]\n",
    "# nrealizations   = [10]\n",
    "\n",
    "def set_histogram(hamiltonian, hist_sig_x: HistogramAverage, hist_sig_z: HistogramAverage, nbins):\n",
    "    bandwidth       = hamiltonian.get_bandwidth()\n",
    "    omax            = bandwidth * 3\n",
    "    omin            = 0.1 / hamiltonian.hilbert_size\n",
    "    tolerance       = 0.02 * bandwidth\n",
    "    hist_sig_x.reset(nbins=nbins)\n",
    "    hist_sig_x.uniform_log(v_max=omax, v_min=omin)\n",
    "    hist_sig_z.reset(nbins=nbins)\n",
    "    hist_sig_z.uniform_log(v_max=omax, v_min=omin)\n",
    "    return tolerance\n",
    "\n",
    "for ii, ns in enumerate(nss):\n",
    "    realizations    = nrealizations[ii] if ii < len(nrealizations) else nrealizations[-1]\n",
    "    energies[ns]    = []\n",
    "    diagonals_x[ns] = []\n",
    "    diagonals_z[ns] = []\n",
    "    nh              = 2**ns\n",
    "    # sites           = [0]\n",
    "    sites           = [ns - 1]\n",
    "    # sites           = [0]\n",
    "    sig_x_op        = sig_x(ns = ns, type_act = OperatorTypeActing.Global, sites = sites)\n",
    "    sig_z_op        = sig_z(ns = ns, type_act = OperatorTypeActing.Global, sites = sites)\n",
    "    sig_x_op_mat    = sig_x_op.matrix(dim = nh, matrix_type = 'sparse', use_numpy = True)\n",
    "    sig_z_op_mat    = sig_z_op.matrix(dim = nh, matrix_type = 'sparse', use_numpy = True)\n",
    "    histogram_sig_x = HistogramAverage(dtype=dtype)\n",
    "    histogram_sig_z = HistogramAverage(dtype=dtype)\n",
    "    nbins           = int(20 * np.log2(nh))\n",
    "    tolerance       = 0.0\n",
    "    for realization in tqdm(range(realizations)):\n",
    "        # print(f\"Realization {realization + 1}/{nrealizations}\")\n",
    "        hamil       = QSM(ns=ns, hilbert_space=None, n=3, a=alpha, h=1.0, xi=0.2, backend = 'np', dtype='float64')\n",
    "        hamil.build(verbose=False, use_numpy=True)\n",
    "        hamil.diagonalize(verbose=False)\n",
    "        states      = hamil.eig_vec\n",
    "        eigvals     = hamil.eig_val\n",
    "        sig_x_elems = overlap(states, states, sig_x_op_mat, backend = backend)\n",
    "        sig_z_elems = overlap(states, states, sig_z_op_mat, backend = backend)\n",
    "        diag_sig_x  = sig_x_elems.diagonal()\n",
    "        diag_sig_z  = sig_z_elems.diagonal()\n",
    "        if realization == 0:\n",
    "            tolerance = set_histogram(hamil, histogram_sig_x, histogram_sig_z, nbins)\n",
    "        # get the average energy\n",
    "        energy_at       = hamil.av_en\n",
    "        # create the f_functions by taking the middle spectrum (%)\n",
    "        w, i_idx, j_idx = Fraction.spectral_function_fraction(eigvals, energy_at, tolerance)\n",
    "        ones            = np.ones_like(sig_x_elems)\n",
    "        ones[i_idx, j_idx] = 0\n",
    "        plt.imshow(ones)\n",
    "        # fill the histogram\n",
    "        sig_x_changed   = np.abs(sig_x_elems[i_idx, j_idx])**2\n",
    "        sig_z_changed   = np.abs(sig_z_elems[i_idx, j_idx])**2\n",
    "        histogram_sig_x.append(w, sig_x_changed)\n",
    "        histogram_sig_z.append(w, sig_z_changed)\n",
    "        diagonals_x[ns].append(diag_sig_x)\n",
    "        diagonals_z[ns].append(diag_sig_z)\n",
    "        energies[ns].append(eigvals)\n",
    "    histograms_x[ns] = histogram_sig_x\n",
    "    histograms_z[ns] = histogram_sig_z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_python.physics.eigenlevels import gap_ratio\n",
    "fig, ax     = Plotter.get_subplots(nrows=2, ncols=2, sizex=5, sizey=5, dpi=150)\n",
    "\n",
    "# plot\n",
    "for ii, ns in enumerate(energies.keys()):\n",
    "    realizations = nrealizations[ii] if ii < len(nrealizations) else nrealizations[-1]\n",
    "    nh          = 2**ns\n",
    "    x           = energies[ns][0] / ns\n",
    "    # x           = np.arange(nh)\n",
    "    colors1, _, _= Plotter.get_colormap(nss, cmap='viridis')\n",
    "    colors2, _, _= Plotter.get_colormap(nss, cmap='plasma_r')\n",
    "    color1      = colors1(ns)\n",
    "    color2      = colors2(ns)\n",
    "    Plotter.scatter(ax[0], x, diagonals_x[ns][0], label=f'$L={ns}$', color=color1)\n",
    "    Plotter.scatter(ax[1], x, diagonals_z[ns][0], label=f'$L={ns}$', color=color2)\n",
    "    Plotter.set_ax_params(ax[0], xlabel='$i/L$', ylabel=r'$O_{ii}$', title=r'$\\sigma_x$')\n",
    "    Plotter.set_ax_params(ax[1], xlabel='$i/L$', ylabel=r'$O_{ii}$', title=r'$\\sigma_z$')\n",
    "\n",
    "    Plotter.scatter(ax[2], histograms_x[ns].bin_edges, histograms_x[ns].averages_av()*nh, label=f'$L={ns}$', color=color1)\n",
    "    Plotter.scatter(ax[3], histograms_z[ns].bin_edges, histograms_z[ns].averages_av()*nh, label=f'$L={ns}$', color=color2)\n",
    "    Plotter.set_ax_params(ax[2], xlabel=r'$\\omega$', lim = {'y': (1e-4, 1e2)},\n",
    "        ylabel=r'$\\mathcal{D}|O_{ij}|^2$', scale={'x': 'log', 'y': 'log'})\n",
    "    Plotter.set_ax_params(ax[3], xlabel=r'$\\omega$', lim = {'y': (1e-4, 1e2)},\n",
    "        ylabel=r'$\\mathcal{D}|O_{ij}|^2$', scale={'x': 'log', 'y': 'log'})\n",
    "    gap_ratios = [gap_ratio(energies[ns][i])['mean'] for i in range(realizations)]\n",
    "    print(f\"ns = {ns}, gap_ratio = {np.mean(gap_ratios)}\")\n",
    "    Plotter.set_legend(ax[2])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_en = np.diff(energies[8])\n",
    "d_en = d_en / np.mean(d_en)\n",
    "gap_ratios = np.minimum(d_en[:-1], d_en[1:]) / np.maximum(d_en[:-1], d_en[1:])\n",
    "print(np.mean(gap_ratios))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_python.algebra as algebra\n",
    "a = algebra.AlgebraTests(backend='jax')\n",
    "b = algebra.AlgebraTests(backend='numpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the basis with a unitary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.change_basis(verbose=True)\n",
    "b.change_basis(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the basis of a matrix with a unitary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.change_basis_matrix(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.outer(verbose=True)\n",
    "b.outer(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kronecker product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.kron(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test through examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = algebra.SolversTests(backend='numpy')\n",
    "b = algebra.SolversTests(backend='jax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.solver_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05_05_2025_18-46_29 [INFO] Log file created: ./log/global_05_05_2025_18-46_29.log\n",
      "05_05_2025_18-46_29 [INFO] Log level set to: info\n",
      "05_05_2025_18-46_29 [INFO] ############Global logger initialized.############\n"
     ]
    }
   ],
   "source": [
    "from general_python.common import binary\n",
    "NUM         = 42\n",
    "tests       = binary.BinaryFunctionTests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "integer     = np.int64(255)\n",
    "integer     = 255\n",
    "ns          = 8\n",
    "i           = 0\n",
    "pos         = ns - 1 - i\n",
    "checked     = binary.check_int(integer, pos)\n",
    "\n",
    "binary.int2binstr(integer, ns), checked, (1 << pos), binary.int2binstr((1 << pos), ns), integer & (1 << pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.run_tests(NUM, spin_value = binary._BACKENDREPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.run_tests(NUM, spin_value = binary._BACKENDREPR, backend = 'np')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algebra import get_backend\n",
    "import time \n",
    "from jax import numpy as jnp, random as jnpr\n",
    "import numpy as np, numpy.random as npr\n",
    "from jax import jit\n",
    "\n",
    "seed = 1701\n",
    "\n",
    "# Test the speed of the matrix multiplication\n",
    "key_np = npr.seed(seed)\n",
    "key_jnp = jnpr.key(seed)\n",
    "size = 5000\n",
    "\n",
    "def matrix_mult_test_jax(size=size):\n",
    "    random_mat  = jnpr.normal(key_jnp, (size, size))\n",
    "    vec         = jnpr.normal(key_jnp, (size,))\n",
    "    result      = random_mat @ vec\n",
    "    print(\"Result of matrix-vector multiplication:\\n\", result)\n",
    "    \n",
    "def matrix_mult_test_numpy(size = size):\n",
    "    random_mat  = npr.normal(size=(size, size))\n",
    "    vec         = npr.normal(size=(size,))\n",
    "    result      = random_mat @ vec\n",
    "    print(\"Result of matrix-vector multiplication:\\n\", result)\n",
    "    \n",
    "@jit\n",
    "def matrix_mult_test_jax_jit():\n",
    "    random_mat  = jnpr.normal(key_jnp, (size, size))\n",
    "    vec         = jnpr.normal(key_jnp, (size,))\n",
    "    result      = random_mat @ vec\n",
    "    print(\"Result of matrix-vector multiplication:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "matrix_mult_test_numpy()\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.4f} seconds (numpy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "matrix_mult_test_jax()\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.4f} seconds (jax)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jax jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "matrix_mult_test_jax_jit()\n",
    "end_time = time.time()  \n",
    "print(f\"Time taken: {end_time - start_time:.4f} seconds (jax, jitted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lattice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lattices import Lattice, run_lattice_tests\n",
    "\n",
    "# Run tests for different configurations\n",
    "# run_lattice_tests(dim=1, lx=10, ly=1, lz=1, bc=lattices.LatticeBC.PBC, typek=\"square\")\n",
    "# run_lattice_tests(dim=2, lx=5, ly=3, lz=1, bc=lattices.LatticeBC.PBC, typek=\"square\")\n",
    "# run_lattice_tests(dim=2, lx=5, ly=5, lz=1, bc=lattices.LatticeBC.OBC, typek=\"square\")\n",
    "# run_lattice_tests(dim=3, lx=3, ly=3, lz=3, bc=lattices.LatticeBC.PBC, typek=\"square\")\n",
    "run_lattice_tests(dim=2, lx=3, ly=2, lz=1, bc=lattices.LatticeBC.PBC, typek=\"honeycomb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "a = 1\n",
    "b = 1\n",
    "c = 1\n",
    "\n",
    "Lx = 4\n",
    "Ly = 2\n",
    "Ns = 2 * Lx * Ly\n",
    "\n",
    "_a1 = np.array([np.sqrt(3) * a / 2.0, 3 * a / 2.0, 0])\n",
    "_a2 = np.array([np.sqrt(3) * a / 2.0, -3 * a / 2.0, 0])\n",
    "_a3 = np.array([0, 0, c])\n",
    "\n",
    "\n",
    "\n",
    "coords      = []\n",
    "move_vector = _a1 - np.array([0, a, 0])\n",
    "for i in range(Ns):\n",
    "    # get the unit cell on a square lattic\n",
    "    x = (i // 2) % Lx\n",
    "    y = ((i//2) // Lx) % Ly\n",
    "    \n",
    "    # from coorditates to position on the lattice\n",
    "    xy = np.array([0., 0, 0]) if i % 2 == 0 else move_vector\n",
    "    xy += x * (_a1 + _a2) + y * _a1\n",
    "    xp, yp = xy[0], xy[1]\n",
    "    print(xp, yp)\n",
    "    plt.scatter(xp, yp, color='red' if i % 2 == 0 else 'blue', marker='o', s=100, facecolor='none')\n",
    "    plt.text(xp + 0.1, yp + 0.1, i)\n",
    "    \n",
    "    print(\"Ns=\", i, \"-- (x,y)=\", x, y)\n",
    "\n",
    "\n",
    "plt.arrow(0, 0, _a1[0], _a1[1], head_width=0.1, head_length=0.1, fc='r', ec='r')\n",
    "plt.arrow(0, 0, _a2[0], _a2[1], head_width=0.1, head_length=0.1, fc='b', ec='b')\n",
    "# plt.arrow(0, 0, _a1[0] - _a2[0], _a1[1] - _a2[1], head_width=0.1, head_length=0.1, fc='g', ec='g')\n",
    "# plt.arrow(0, 0, _a1[0] + _a2[0], _a1[1] + _a2[1], head_width=0.1, head_length=0.1, fc='y', ec='y')\n",
    "# plt.arrow(0, 0, -_a1[0] + _a2[0], -_a1[1] + _a2[1], head_width=0.1, head_length=0.1, fc='m', ec='m')\n",
    "# plt.arrow(0, 0, -_a1[0] - _a2[0], -_a1[1] - _a2[1], head_width=0.1, head_length=0.1, fc='c', ec='c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NQS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler\n",
    "\n",
    "<!-- #### Code Overview\n",
    "\n",
    "This code sets up a computational experiment by combining a simple neural network with Monte Carlo sampling. Below is a breakdown of the major steps and components.\n",
    "\n",
    "---\n",
    "\n",
    "##### 1. Module Imports\n",
    "\n",
    "- **Monte Carlo Sampling:**  \n",
    "  The module `Solver.MonteCarlo.sampler` is imported as `Sampling` to provide Monte Carlo sampling functionality.\n",
    "\n",
    "- **Backend Utilities:**  \n",
    "  `get_backend` from `general_python.algebra.utils` is used to select a computational backend (e.g., NumPy) and obtain associated random number generators.\n",
    "\n",
    "- **Neural Network Implementation:**  \n",
    "  `SimpleNet` from `general_python.ml.net_impl.net_simple` is imported as a basic neural network class used for experimentation.\n",
    "\n",
    "- **NumPy:**  \n",
    "  Imported as `np` to support numerical operations.\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. Backend Setup\n",
    "\n",
    "- **Selection & Seed:**\n",
    "  - **Backend:** Set to `\"np\"` (NumPy).\n",
    "  - **Seed:** A seed of `1701` is specified to ensure reproducibility.\n",
    "\n",
    "- **Obtaining Backend Modules:**  \n",
    "  The function `get_backend` is called with parameters to ensure randomness (and optionally SciPy support). Its output is unpacked into:\n",
    "  - `backend_np`: The NumPy backend.\n",
    "  - `(rng, rng_k)`: Random number generators.\n",
    "  - `backend_sp`: The SciPy backend (if available).\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. Neural Network Configuration\n",
    "\n",
    "- **State Shape:**  \n",
    "  The input to the network is defined as a 10-dimensional vector (`st_shape = (10,)`).\n",
    "\n",
    "- **Activation Functions:**  \n",
    "  Two activation functions are specified: `'relu'` and `'sigmoid'`.\n",
    "\n",
    "- **Network Initialization:**  \n",
    "  A `SimpleNet` instance is created with:\n",
    "  - **Input Shape:** `(10,)`\n",
    "  - **Output Shape:** `(1,)`\n",
    "  - **Layers:** A tuple `(5,)` indicating the network has 5 layers.\n",
    "  - **Backend:** Specified as `'np'`.\n",
    "  - **Data Type:** `'complex128'`, enabling complex-valued computations.\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. Monte Carlo Sampler Configuration\n",
    "\n",
    "- **Sampler Initialization:**  \n",
    "  An instance of `Sampling.MCSampler` is created to perform Monte Carlo sampling on the neural network.\n",
    "\n",
    "- **Key Parameters:**\n",
    "  - **Network (`net`):** The previously defined `SimpleNet` instance.\n",
    "  - **Shape:** The state shape `(10,)`.\n",
    "  - **Random Number Generators:** `rng` and `rng_k` ensure randomness in the sampling process.\n",
    "  - **Chains & Samples:** Configured to run 10 chains with 5 samples each.\n",
    "  - **Sweep Steps & Thermalization:** Set to 10 steps each, controlling the progression and stabilization of the Monte Carlo simulation.\n",
    "  - **Backend:** Uses the specified NumPy backend.\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. Final Outcome\n",
    "\n",
    "The final product is the `sampler` object, which encapsulates both the neural network and the Monte Carlo sampling parameters. This setup is ready to run experiments, allowing exploration of the network's state space through stochastic sampling.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "This code initializes a complex-valued neural network and configures a Monte Carlo sampler to perform experiments. It ensures reproducibility with a fixed seed and leverages a specified backend (NumPy) along with associated random number generators, thereby preparing the system for robust computational experiments. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Solver.MonteCarlo.sampler as Sampling\n",
    "from general_python.algebra.utils import get_backend\n",
    "from general_python.ml.net_impl.net_simple import SimpleNet\n",
    "import numpy as np\n",
    "\n",
    "# get random backend\n",
    "backend                                 = 'jax'\n",
    "seed                                    = 1701\n",
    "be_modules                              = get_backend(backend, random=True, seed=seed, scipy=True)\n",
    "backend_np, (rng, rng_k), backend_sp    = be_modules if isinstance(be_modules, tuple) else (be_modules, (None, None), None)\n",
    "\n",
    "ns                                      = 10\n",
    "st_shape                                = (ns, )\n",
    "activations                             = ('relu', 'sigmoid')\n",
    "net                                     = SimpleNet(act_fun =   activations, \n",
    "                                            input_shape     =   st_shape, \n",
    "                                            output_shape    =   (1, ),\n",
    "                                            layers          =   (5, ),\n",
    "                                            backend=backend, dtype='complex128',)\n",
    "            # [random.random() + 1j * random.random() for _ in range(x.shape[0])])\n",
    "# a simple callable network function\n",
    "\n",
    "sampler = Sampling.MCSampler(\n",
    "    net         = net,\n",
    "    shape       = st_shape,\n",
    "    rng         = rng,\n",
    "    rng_k       = rng_k,\n",
    "    numchains   = 10,\n",
    "    numsamples  = 5,\n",
    "    sweep_steps = 10,\n",
    "    backend     = backend,\n",
    "    therm_steps = 10,\n",
    ")\n",
    "\n",
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how the sampler performs\n",
    "(states, logprobas), (configs, configs_ansatze), probabilities = sampler.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final states after sampling, ansatze after sampling\n",
    "states.shape, logprobas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.shape, configs_ansatze.shape, probabilities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_python.common.plot import MatrixPrinter\n",
    "\n",
    "print(\"Detailed Flow of the Configurations:\\n\")\n",
    "print(f\"  Total number of configurations: {len(configs)}\")\n",
    "print(f\"  Total number of states: {len(states)}\")\n",
    "print(f\"  Total number of probabilities: {len(probabilities)}\\n\")\n",
    "\n",
    "for i in range(len(configs[:3])):\n",
    "    print(f\"--- Configuration flow {i} ---\")\n",
    "    print(\"     Configuration \\\\(\\\\mathbf{S}_{\" + f\"{i}\" + \"}\\\\):\")\n",
    "    MatrixPrinter.print_vector(configs[i])\n",
    "    \n",
    "    print(\"    Ansatz \\\\(f(\\\\mathbf{S}_{\" + f\"{i}\" + \"})\\\\):\")\n",
    "    MatrixPrinter.print_vector(configs_ansatze[i].T)\n",
    "    \n",
    "    print(\"    Probability \\\\(P(\\\\mathbf{S}_{\" + f\"{i}\" + \"})\\\\):\")\n",
    "    MatrixPrinter.print_vector(probabilities[i].T)\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_python.common.plot import MatrixPrinter\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Flatten the configurations and ansatz arrays with a clearer mathematical description\n",
    "#\n",
    "# Let   ^(Nd) represent the configuration matrix, where:\n",
    "#   N = number of samples (configurations)\n",
    "#   d = st_shape[0] (the dimension of each configuration)\n",
    "#\n",
    "# Similarly, for the ansatz values f() (one per configuration), we flatten the array.\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Reshape configurations to a 2D array: each row is one configuration vector.\n",
    "configs_flat = configs.reshape(-1, st_shape[0])\n",
    "print(\"Shape of flattened configurations ():\", configs_flat.shape)\n",
    "\n",
    "# Flatten the ansatze: each configuration has a single ansatz value.\n",
    "configs_ansatze_flat = configs_ansatze.reshape(-1)\n",
    "print(\"Shape of flattened ansatze (f()):\", configs_ansatze_flat.shape)\n",
    "\n",
    "print(\"\\nDetailed flow of the configurations:\")\n",
    "print(\"Number of configurations:\", len(configs_flat))\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# For the first 5 configurations, print:\n",
    "#   - The configuration vector \n",
    "#   - The ansatz from the network f()\n",
    "#   - A verification whether f() matches the ansatz given by the sampler.\n",
    "# -----------------------------------------------------------------\n",
    "for i in range(len(configs_flat[:5])):\n",
    "    print(f\"--- Configuration {i} ---\")\n",
    "    \n",
    "    print(\"\\tConfiguration ():\")\n",
    "    print(\"\\t\", configs_flat[i])\n",
    "    # MatrixPrinter.print_vector(configs_flat[i]\n",
    "    \n",
    "    print(\"'\\tAnsatz from sampler (f()):\")\n",
    "    # Compute the ansatz from the network using the configuration.\n",
    "    print(\"\\t\", configs_ansatze_flat[i])\n",
    "    ansatz_net = net(configs_flat[i])\n",
    "    print(\"\\tAnsatz from the network:\")\n",
    "    print(\"\\t\", ansatz_net)\n",
    "    \n",
    "    # Verify if the network's ansatz equals the sampler's ansatz.\n",
    "    is_same = np.allclose(ansatz_net, configs_ansatze_flat[i])\n",
    "    print(\"\\tVerification (f() from network ?= sampler's ansatz):\", is_same)\n",
    "    \n",
    "    print(\"\\n-------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver class\n",
    "\n",
    "Below, we:\n",
    "\n",
    "1. Create a simple network for the NQS ansatz\n",
    "2. Create a sampler\n",
    "3. Create a dummy Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NQS.nqs as nqsmodule\n",
    "import Solver.MonteCarlo.sampler as Sampling\n",
    "from Algebra.Model.dummy import DummyHamiltonian\n",
    "from general_python.algebra.utils import get_backend\n",
    "from general_python.ml.net_impl.net_simple import SimpleNet\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# get random backend\n",
    "# backend             = 'np'\n",
    "backend             = 'jax'\n",
    "seed                = 1701\n",
    "dtypex              = complex\n",
    "be_modules          = get_backend(backend, random=True, seed=seed, scipy=True)\n",
    "backend_np, (rng, rng_k), backend_sp = be_modules if isinstance(be_modules, tuple) else (be_modules, (None, None), None)\n",
    "\n",
    "# get the network\n",
    "ns                  = 20\n",
    "st_shape            = (ns, )\n",
    "activations         = ('relu', 'sigmoid')\n",
    "net                 = SimpleNet(act_fun =   activations, \n",
    "                        input_shape     =   st_shape, \n",
    "                        output_shape    =   (1, ),\n",
    "                        layers          =   (5, ),\n",
    "                        backend         =   backend_np,\n",
    "                        dtype           =   dtypex)\n",
    "\n",
    "# get the sampler\n",
    "sampler             = Sampling.MCSampler(\n",
    "                            net         = net,\n",
    "                            shape       = st_shape,\n",
    "                            rng         = rng,\n",
    "                            rng_k       = rng_k,\n",
    "                            numchains   = 10,\n",
    "                            numsamples  = 5,\n",
    "                            sweep_steps = 30,\n",
    "                            backend     = backend_np,\n",
    "                            therm_steps = 10,\n",
    "                        )\n",
    "\n",
    "# get the Hamiltonian\n",
    "# ham_dtype           = complex\n",
    "ham_dtype           = float\n",
    "par                 = 0.5 + 1j * 0.5 if ham_dtype == complex else 0.5\n",
    "ham                 = DummyHamiltonian(hilbert_space=None, ns = ns, backend = backend, dtype = ham_dtype)\n",
    "batch_size          = 1\n",
    "\n",
    "nqs                 = nqsmodule.NQS(\n",
    "                            net         = net,\n",
    "                            sampler     = sampler,\n",
    "                            hamiltonian = ham,\n",
    "                            lower_betas = None,\n",
    "                            lower_states= None,\n",
    "                            seed        = seed,\n",
    "                            beta        = 1.0,\n",
    "                            shape       = st_shape,\n",
    "                            backend     = backend,\n",
    "                            nthread     = 1,\n",
    "                            batch_size  = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(states, logprobas), (configs, configs_ansatze), probabilities = nqs.sample(num_samples=5, num_chains=5)\n",
    "configs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training.train_state import TrainState\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "\n",
    "# try to train the network\n",
    "\n",
    "n_epo       = 150\n",
    "batch_size  = 10\n",
    "history     = []\n",
    "sr          = nqs.sr\n",
    "# Initialize the optimizer\n",
    "# lr          = optax.linear_schedule(init_value=0.001, end_value=0.0001, transition_steps=1000)\n",
    "# optimizer   = optax.adam(learning_rate=lr)\n",
    "lr          = 6e-3\n",
    "# Create a TrainState to manage the model parameters and optimizer state\n",
    "# train_state = TrainState.create(apply_fn=net.apply, params=net.get_params(), tx=optimizer)\n",
    "\n",
    "for i in range(n_epo):\n",
    "    print(f\"Epoch {i + 1}/{n_epo}\")\n",
    "    \n",
    "    # collect the samples\n",
    "    (states, logprobas), (configs, configs_ansatze), probabilities = nqs.sample()\n",
    "    print(f\"  Number of configurations: {configs.shape}\")\n",
    "    print(f\"  Configs and ansatze: {configs_ansatze.shape}\")\n",
    "    print(f\"  Number of states: {states.shape}\")\n",
    "    print(f\"  Number of probabilities: {probabilities.shape}\\n\")\n",
    "    \n",
    "    # collect the energies\n",
    "    (configs, ansatze), probabilities, (v, means, stds) = nqs.evaluate_fun(\n",
    "        states_and_psi  = (configs, configs_ansatze),\n",
    "        probabilities   = probabilities,\n",
    "        functions       = None,     # means that we evaluate energy\n",
    "        batch_size      = batch_size\n",
    "    )\n",
    "    # print(f\"  Energies: {v.shape}\")\n",
    "    # print(f\"  Mean energy: {means}, std: {stds}\")\n",
    "    # print(f\"  Configurations: {configs.shape}\")\n",
    "    \n",
    "    # get the gradients\n",
    "    # grad = nqs.gradient(states=configs, batch_size=batch_size)\n",
    "    \n",
    "    # try to calculate the stochastic reconfiguration\n",
    "    # sr.set_values(loss=v, derivatives=grad, mean_loss=means, mean_deriv=None, calculate_s=True, use_minsr=False)\n",
    "\n",
    "    # f = sr.forces\n",
    "    # s = sr.covariance_matrix\n",
    "    # print(f\"  Forces: {f.shape}\")\n",
    "    # if s is not None:\n",
    "        # print(f\"  Covariance matrix: {s.shape}\")\n",
    "    \n",
    "    # try to calculate the gradient through the stochastic reconfiguration\n",
    "    # s_inv   = jnp.linalg.pinv(s, rtol = 1e-3)\n",
    "    # print(f\"  Inverse covariance matrix: {s_inv.shape}\")\n",
    "    # df      = lr * jnp.dot(s_inv, f)\n",
    "    # df = jnp.linalg.solve(s, f)\n",
    "    # print(f\"  Gradient through the stochastic reconfiguration: {df.shape}\")\n",
    "    \n",
    "    # Update the network parameters using TrainState\n",
    "    \n",
    "    # new_params      = train_state.apply_gradients(grads={\"params\":df})\n",
    "    # nqs.update_parameters(df)\n",
    "    # updates, new_opt_state  = train_state.tx.update(df, train_state.opt_state, train_state.params)\n",
    "    # new_params              = optax.apply_updates(train_state.params, updates)\n",
    "    # train_state             = train_state.replace(params=new_params, opt_state=new_opt_state)\n",
    "    \n",
    "    # history.append(means)\n",
    "    \n",
    "\n",
    "plt.plot(history)\n",
    "\n",
    "# Number of configurations: (25, 10)\n",
    "# Configs and ansatze: (25, 1)\n",
    "# Number of states: (5, 10)\n",
    "# Number of probabilities: (25, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test flax networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j                   = -1.0   \n",
    "g                   = -0.5   \n",
    "h                   = 0.5\n",
    "ham                 =   TransverseFieldIsing(\n",
    "    lattice         = lattice,\n",
    "    hilbert_space   = None,\n",
    "    j               = j,\n",
    "    hz              = h,\n",
    "    hx              = g,\n",
    "    dtype           = ham_dtype,\n",
    "    backend         = backend_np\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_python.common.binary as binary\n",
    "\n",
    "int_state       = np.random.randint(0, 2**ham.ns)\n",
    "print(f\"Integer state: {int_state}\")\n",
    "loc_energy_int  = []\n",
    "for i in range(ham.ns):\n",
    "    loc_energy_int.append(ham.loc_energy_int(int_state, i))\n",
    "loc_states_int  = [i[0] for i in loc_energy_int]\n",
    "loc_energy_int  = [i[1] for i in loc_energy_int]\n",
    "print(loc_states_int)\n",
    "# flatten the list\n",
    "loc_energy_int  = [item for sublist in loc_energy_int for item in sublist]\n",
    "loc_states_int  = [item for sublist in loc_states_int for item in sublist]\n",
    "loc_energy_int_l= [loc_energy_int[i] for i in range(len(loc_energy_int)) if loc_states_int[i] == int_state]\n",
    "loc_states_int_l= [loc_states_int[i] for i in range(len(loc_energy_int)) if loc_states_int[i] == int_state]\n",
    "loc_energy_int  = [loc_energy_int[i] for i in range(len(loc_states_int)) if loc_states_int[i] != int_state]\n",
    "loc_states_int  = [loc_states_int[i] for i in range(len(loc_states_int)) if loc_states_int[i] != int_state]\n",
    "loc_energy_int  = [np.sum(loc_energy_int_l)] + loc_energy_int \n",
    "loc_states_int  = [int_state] + loc_states_int\n",
    "\n",
    "jax_state       = binary.int2base(n = int_state, size = ham.ns, backend = 'np')\n",
    "jax_state       = jnp.array(jax_state, dtype=jnp.float64)\n",
    "loc_energy_jax  = ham.loc_energy_arr_jax(jax_state)\n",
    "loc_states_jax  = loc_energy_jax[0]\n",
    "loc_energy_jax  = loc_energy_jax[1]\n",
    "\n",
    "print(len(loc_energy_int), len(loc_energy_jax))\n",
    "print(len(loc_states_int), len(loc_states_jax))\n",
    "for i in range(len(loc_energy_int)):\n",
    "    print(\"i=\", i)\n",
    "    print(f\"\\tInt state: {loc_states_int[i]}, Energy: {loc_energy_int[i]}\")\n",
    "    print(f\"\\tInt state: {binary.base2int(loc_states_jax[i])}, Jax state: {loc_states_jax[i]}, Energy: {loc_energy_jax[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient\n",
    "jax_state_reshape = jax_state.reshape(1, -1)\n",
    "# jax_state_reshape = jnp.array([jax_state, jax_state], dtype=jnp.float64)\n",
    "print(jax_state_reshape.shape)\n",
    "\n",
    "# get the gradient - numerical\n",
    "grad_num, shapes, sizes, is_cpx = nqs.gradient(states=jax_state_reshape, batch_size=1, params=net.get_params())\n",
    "grad_num                = nqs.transform_flat_params(grad_num[0], shapes, sizes, is_cpx)\n",
    "# print(\"Gradient - numerical:\", grad_num)\n",
    "# get the gradient - analytical\n",
    "grad_anal_f, params = net.get_gradient()\n",
    "grad_anal           = grad_anal_f(net.get_params(), jax_state_reshape)\n",
    "# print(\"Gradient - analytical:\", grad_anal)\n",
    "\n",
    "# compare\n",
    "for g, g_ in zip(grad_num, grad_anal):\n",
    "    print(f\"\\nGradient Group: {g}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    elem_num = grad_num[g]\n",
    "    elem_anal = grad_anal[g_]\n",
    "    \n",
    "    if 'bias' in elem_num:\n",
    "        bias_num = elem_num['bias']\n",
    "        bias_anal = elem_anal['bias'][0]\n",
    "        print(\"\\tBias - Numerical:\")\n",
    "        print(f\"\\t{bias_num}\")\n",
    "        print(f\"\\tShape: {bias_num.shape}, dtype: {bias_num.dtype}\")\n",
    "        print(\"\\tBias - Analytical:\")\n",
    "        print(f\"\\t{bias_anal}\")\n",
    "        print(f\"\\tShape: {bias_anal.shape}, dtype: {bias_anal.dtype}\")\n",
    "        bias_diff = bias_num - bias_anal\n",
    "        print(\"\\tBias Difference:\")\n",
    "        print(f\"\\t{bias_diff}\")\n",
    "    else:\n",
    "        print(f\"Numerical: {elem_num}\")\n",
    "    \n",
    "    if 'kernel' in elem_num:\n",
    "        kernel_num = elem_num['kernel']\n",
    "        kernel_anal = elem_anal['kernel'][0]\n",
    "        print(\"\\tKernel - Numerical:\")\n",
    "        print(f\"\\t{kernel_num}\")\n",
    "        print(f\"\\tShape: {kernel_num.shape}, dtype: {kernel_num.dtype}\")\n",
    "        print(\"\\tKernel - Analytical:\")\n",
    "        print(f\"\\t{kernel_anal}\")\n",
    "        print(f\"\\tShape: {kernel_anal.shape}, dtype: {kernel_anal.dtype}\")\n",
    "        kernel_diff = kernel_num - kernel_anal\n",
    "        print(\"\\tKernel Difference:\")\n",
    "        print(f\"\\t{kernel_diff}\")\n",
    "    else:\n",
    "        print(f\"Analytical: {elem_anal}\")\n",
    "        print(f\"Numerical: {elem_num}\")\n",
    "        print(f\"Difference: {elem_anal - elem_num}\")\n",
    "    \n",
    "    # print(\"\\nSummary for Group:\")\n",
    "    # print(f\"\\tNumerical: {elem_num}\")\n",
    "    # print(f\"\\tAnalytical: {elem_anal}\")\n",
    "    # print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_anal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_python.common.binary as binary\n",
    "\n",
    "def do():\n",
    "    (states, logprobas), (configs, configs_ansatze), probabilities = sampler.sample(num_samples=5, num_chains=5)\n",
    "    for config in configs[:2]:\n",
    "        print(config)\n",
    "        print(binary.base2int(config))\n",
    "        print()\n",
    "%timeit -r 5 -n 5 do()\n",
    "(states, logprobas), (configs, configs_ansatze), probabilities = sampler.sample(num_samples=5, num_chains=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_ansatze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.apply(net.get_params(), configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham.loc_energy_arr_jax(configs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(configs, ansatze), probabilities, (v, means, stds) = nqs.evaluate_fun(\n",
    "    states_and_psi  = (configs, configs_ansatze),\n",
    "    probabilities   = probabilities,\n",
    "    functions       = None,     # means that we evaluate energy\n",
    "    batch_size      = batch_size\n",
    ")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = nqs.get_params().copy()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = net.get_params().copy()\n",
    "params['visible_bias'] = jnp.array([0.1] * ns, dtype=params['visible_bias'].dtype)\n",
    "nqs.update_parameters(params)\n",
    "print(params['visible_bias'])\n",
    "print(net.get_params()['visible_bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(configs, ansatze), probabilities, (v, means, stds) = nqs.evaluate_fun(\n",
    "    states_and_psi  = (configs, configs_ansatze),\n",
    "    probabilities   = probabilities,\n",
    "    functions       = None,     # means that we evaluate energy\n",
    "    batch_size      = batch_size\n",
    ")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new parameters\n",
    "net.set_params(params=p)\n",
    "(configs, ansatze), probabilities, (v, means, stds) = nqs.evaluate_fun(\n",
    "    states_and_psi  = (configs, configs_ansatze),\n",
    "    probabilities   = probabilities,\n",
    "    functions       = None,     # means that we evaluate energy\n",
    "    batch_size      = batch_size\n",
    ")\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PINV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_python.algebra.solvers as solvers \n",
    "\n",
    "solver_linalg = solvers.choose_solver(solver_id=solvers.SolverType.PSEUDO_INVERSE)\n",
    "solver_linalg_func = solver_linalg.get_solver_func(backend_module=backend_np)\n",
    "solver_linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "n               = 100\n",
    "random_matrix   = jnp.array(npr.normal(size=(n, n)), dtype=jnp.float64)\n",
    "matrix          = (random_matrix + random_matrix.T) / 2  # Make the matrix symmetric\n",
    "vector          = jnp.array(npr.normal(size=(n,)), dtype=jnp.float64)\n",
    "\n",
    "@jax.jit\n",
    "def matvec(x):\n",
    "    return jnp.dot(matrix, x)\n",
    "reg      = 1e-10\n",
    "tol      = 1e-3\n",
    "solution = solver_linalg_func(matvec=matvec, b=vector, x0=None,\n",
    "                            tol=tol, sigma=reg, A=matrix)\n",
    "%timeit -r 5 -n 5 solver_linalg_func(matvec=matvec, b=vector, x0=None, tol=tol, sigma=reg, A=matrix)\n",
    "# Solve the linear system Ax = b\n",
    "# solution = solver_linalg_func(matrix, vector)\n",
    "matrix @ solution[0] - vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_python.algebra.solvers as solvers \n",
    "import general_python.algebra.preconditioners as preconditioners\n",
    "\n",
    "#! SOLVER\n",
    "solver_linalg       = solvers.choose_solver(solver_id=solvers.SolverType.SCIPY_CG)\n",
    "solver_linalg_func  = solver_linalg.get_solver_func(backend_module=backend_np,\n",
    "                                        use_matvec=False, use_matrix=True, sigma=None)\n",
    "reg                 = 1e-6\n",
    "\n",
    "#! PRECONDITIONER\n",
    "precond             = preconditioners.choose_precond(precond_id =   preconditioners.PreconditionersTypeSym.JACOBI,\n",
    "                                                    backend     =   backend_np)\n",
    "precond.sigma       = reg\n",
    "precond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy.random as npr\n",
    "\n",
    "n               = 100\n",
    "random_matrix   = jnp.array(npr.normal(size=(n, n)), dtype=jnp.float64)\n",
    "matrix          = (random_matrix + random_matrix.T) / 2  # Make the matrix symmetric\n",
    "vector          = jnp.array(npr.normal(size=(n,)), dtype=jnp.float64)\n",
    "\n",
    "#! set the preconditioner\n",
    "precond_func    = precond.get_apply_mat()\n",
    "precond_func    = lambda r, a: r\n",
    "print(precond_func)\n",
    "\n",
    "\n",
    "\n",
    "tol      = 1e-9\n",
    "maxiter  = 2000\n",
    "solution = solver_linalg_func(\n",
    "        a               =   matrix,\n",
    "        precond_apply   =   precond_func,\n",
    "        b               =   vector, \n",
    "        x0              =   jnp.zeros_like(vector),\n",
    "        tol             =   tol, \n",
    "        maxiter         =   maxiter)\n",
    "%timeit -r 10 -n 5 solver_linalg_func(a=matrix, precond_apply=precond_func, b=vector, x0=jnp.zeros_like(vector), tol=tol, maxiter=maxiter)\n",
    "# Solve the linear system Ax = b\n",
    "# solution = solver_linalg_func(matrix, vector)\n",
    "# if solution.converged:\n",
    "    # print(\"Converged\")\n",
    "# else:\n",
    "    # print(\"Did not converge\")\n",
    "print(\"Residual:\", (matrix @ solution[0] - vector)[:5])\n",
    "print(\"My solution:\", solution[0][:5])\n",
    "\n",
    "# solution from jax.scipy\n",
    "from jax.scipy.sparse.linalg import cg\n",
    "from jax import jit\n",
    "\n",
    "@jit\n",
    "def solve_cg(matrix, vector):\n",
    "    return cg(matrix, vector, tol=tol, maxiter=maxiter, M=lambda r: r)\n",
    "\n",
    "solution = solve_cg(matrix, vector)\n",
    "print(\"Solution:\", solution[0][:5])\n",
    "print(\"Residual:\", (matrix @ solution[0] - vector)[:5])\n",
    "\n",
    "%timeit -r 5 -n 5 solve_cg(matrix, vector)\n",
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import general_python.algebra.solvers.stochastic_rcnfg as SR\n",
    "import jax.numpy as jnp\n",
    "from tqdm import trange\n",
    "\n",
    "import general_python.algebra.solvers as solvers\n",
    "import general_python.algebra.preconditioners as preconditioners\n",
    "\n",
    "#! Configuration\n",
    "n_epo           =       500\n",
    "batch_size      =       20\n",
    "lr              =       5e-2\n",
    "pinv            =       1e-8\n",
    "sreg            =       1e-1\n",
    "\n",
    "solver_linalg   =       solvers.choose_solver(solver_id=solvers.SolverType.SCIPY_CG, sigma=sreg)\n",
    "precond         =       preconditioners.choose_precond(precond_id=preconditioners.PreconditionersTypeSym.JACOBI, backend='jax')\n",
    "# precond.sigma   =       sreg\n",
    "precond_apply   =       precond.get_apply_gram()\n",
    "# precond_apply   =       lambda r, s, sp: r\n",
    "\n",
    "# Ensure solver function gets sigma correctly\n",
    "solver_linalg_func = solver_linalg.get_solver_func(backend_module=backend_np,\n",
    "                                        use_matvec  =   False, # Set True if you provide matvec, False if using Fisher S matrix\n",
    "                                        use_matrix  =   False, # Set True to form full S matrix (memory intensive!)\n",
    "                                        use_fisher  =   True,  # Indicates S matrix or matvec is Fisher info\n",
    "                                        sigma       =   sreg)  # Pass regularization here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext line_profiler\n",
    "\n",
    "#! Initialization\n",
    "history                 =       []\n",
    "sampling_times          =       []\n",
    "evaluation_times        =       []\n",
    "gradient_times          =       []\n",
    "reconfiguration_times   =       []\n",
    "update_times            =       []\n",
    "epoch_times             =       [] # To store total time per epoch\n",
    "\n",
    "#! Initial values for display before first epoch calculation\n",
    "means                   =       float('nan')\n",
    "stds                    =       float('nan')\n",
    "\n",
    "def train_function():\n",
    "    #! Training Loop with tqdm\n",
    "    # Use trange outside the loop variable assignment to get the progress bar object\n",
    "    pbar                    = trange(n_epo, desc=\"Starting Training...\", leave=True)\n",
    "    for i in pbar:\n",
    "        epoch_start_time = time.time() # Time the whole epoch\n",
    "\n",
    "        #! 1. Sampling\n",
    "        start_time      =       time.time()\n",
    "        (states, logprobas), (configs, configs_ansatze), probabilities = nqs.sample(reset=False)\n",
    "        sample_time     =       time.time() - start_time\n",
    "        sampling_times.append(sample_time)\n",
    "\n",
    "        #! 2. Energy Evaluation\n",
    "        start_time      =       time.time()\n",
    "        # Need to handle potential errors from evaluate_fun if sampling failed\n",
    "        try:\n",
    "            (configs, ansatze), probabilities, (v, means, stds) = nqs.evaluate_fun(\n",
    "                states_and_psi  = (configs, configs_ansatze),\n",
    "                probabilities   = probabilities,\n",
    "                functions       = None, # means that we evaluate energy\n",
    "                batch_size      = batch_size\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during energy evaluation in epoch {i+1}: {e}\")\n",
    "            print(\"Stopping training.\")\n",
    "            break\n",
    "        eval_time       =       time.time() - start_time\n",
    "        evaluation_times.append(eval_time)\n",
    "\n",
    "        # Check for NaN in energy early\n",
    "        if np.isnan(means):\n",
    "            print(f\"\\nNaN detected in energy calculation at epoch {i+1}, stopping training.\")\n",
    "            # Optionally print some state info for debugging\n",
    "            # print(\"Last configs:\", configs)\n",
    "            break\n",
    "\n",
    "        #! 3. Gradient Computation\n",
    "        start_time      =       time.time()\n",
    "        try:\n",
    "            grad        =       nqs.gradient(states=configs, batch_size=batch_size, params=nqs.get_params())\n",
    "            # print(grad.shape)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during gradient calculation in epoch {i+1}: {e}\")\n",
    "            print(\"Stopping training.\")\n",
    "            break # Exit loop on gradient error\n",
    "        grad_time       =       time.time() - start_time\n",
    "        gradient_times.append(grad_time)\n",
    "\n",
    "        #! 4. Stochastic Reconfiguration\n",
    "        start_time      =       time.time()\n",
    "        try:\n",
    "            # Ensure v (loss) and grad (var_deriv) are correctly shaped and typed\n",
    "            # v should be (n_samples,) real array (E_loc - E_mean)\n",
    "            # grad should be (n_samples, n_params) complex array (O_k or O_k^*)\n",
    "            if not isinstance(v, (np.ndarray, jnp.ndarray)) or v.ndim != 1:\n",
    "                print(f\"\\nWarning: Unexpected format for 'loss' (v) in epoch {i+1}. Shape: {getattr(v, 'shape', 'N/A')}\")\n",
    "            if not isinstance(grad, (np.ndarray, jnp.ndarray)) or grad.ndim != 2:\n",
    "                print(f\"\\nWarning: Unexpected format for 'var_deriv' (grad) in epoch {i+1}. Shape: {getattr(grad, 'shape', 'N/A')}\")\n",
    "\n",
    "            # Ensure x0 matches the *complex* dtype of the expected solution\n",
    "            # SR solution df.x should match gradient complexity\n",
    "            expected_dtype  = grad.dtype\n",
    "            x0_sr           = jnp.zeros_like(grad[0, :], dtype=expected_dtype)\n",
    "            df              =       SR.solve_jax(\n",
    "                solve_func      = solver_linalg_func,\n",
    "                loss            = v,\n",
    "                var_deriv       = grad,\n",
    "                min_sr          = False,\n",
    "                x0              = x0_sr,\n",
    "                precond_apply   = precond_apply,\n",
    "                maxiter         = 500,\n",
    "                tol             = pinv\n",
    "            )\n",
    "            # s_c             =  SR.derivatives_centered_jax(grad, jnp.mean(grad, axis = 0))\n",
    "            # s_c_h           =  jnp.conjugate(s_c.T)\n",
    "            # loss_c          =  SR.loss_centered_jax(v, means)\n",
    "            # df              =  SR.gradient_jax(s_c_h, loss_c, loss_c.shape[0])\n",
    "            # df              =  solvers.SolverResult(x=df, converged=True, residual_norm=None, iterations=None)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during SR calculation in epoch {i+1}: {e}\")\n",
    "            print(f\"SR Input shapes/dtypes: loss={v.shape}/{v.dtype}, var_deriv={grad.shape}/{grad.dtype}\")\n",
    "            print(\"Stopping training.\")\n",
    "            break\n",
    "        reconfig_time       =       time.time() - start_time\n",
    "        reconfiguration_times.append(reconfig_time)\n",
    "\n",
    "        #! 5. Update Network Parameters\n",
    "        start_time          =       time.time()\n",
    "        try:\n",
    "            # The update step: parameters = parameters - learning_rate * df.x\n",
    "            # nqs.update_parameters expects the full step (-lr * df.x)\n",
    "            nqs.update_parameters(df.x, -lr)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during parameter update in epoch {i+1}: {e}\")\n",
    "            print(f\"Update step shape/dtype: {df.x.shape}/{df.x.dtype}\")\n",
    "            print(\"Stopping training.\")\n",
    "            break\n",
    "        update_time         =       time.time() - start_time\n",
    "        update_times.append(update_time)\n",
    "\n",
    "        # --- Store history and update progress bar ---\n",
    "        history.append(means)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        # Update tqdm description and postfix\n",
    "        pbar.set_description(f\"Epoch {i + 1}/{n_epo}\")\n",
    "        postfix_dict = {\n",
    "            \"E_mean\"    : f\"{means:.6f}\",\n",
    "            \"E_std\"     : f\"{stds:.4f}\",\n",
    "            \"t_samp\"    : f\"{sample_time:.3e}s\",\n",
    "            \"t_eval\"    : f\"{eval_time:.3e}s\",\n",
    "            \"t_grad\"    : f\"{grad_time:.3e}s\",\n",
    "            \"t_sr\"      : f\"{reconfig_time:.3e}s\",\n",
    "            \"t_update\"  : f\"{update_time:.3e}s\",\n",
    "            \"t_epoch\"   : f\"{epoch_time:.3e}s\"\n",
    "        }\n",
    "        pbar.set_postfix(postfix_dict, refresh=True)\n",
    "        # print(nqs.get_params())\n",
    "\n",
    "    #! End of Loop\n",
    "    pbar.close()\n",
    "    return history, sampling_times, evaluation_times, gradient_times, reconfiguration_times, update_times, epoch_times\n",
    "\n",
    "\n",
    "# %lprun -f train_function history, sampling_times, evaluation_times, gradient_times, reconfiguration_times, update_times, epoch_times = train_function()\n",
    "# %lprun -f train_function history, sampling_times, evaluation_times, gradient_times, reconfiguration_times, update_times, epoch_times = train_function()\n",
    "\n",
    "history, sampling_times, evaluation_times, gradient_times, reconfiguration_times, update_times, epoch_times = train_function()\n",
    "\n",
    "#! Final Summary\n",
    "if history:\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "    print(f\"Completed {len(history)} epochs.\")\n",
    "    print(f\"Final Mean Energy: {history[-1]:.6f}\")\n",
    "\n",
    "    # Calculate averages only on successful steps\n",
    "    avg_sampling_time           = np.mean(sampling_times) if sampling_times else float('nan')\n",
    "    avg_evaluation_time         = np.mean(evaluation_times) if evaluation_times else float('nan')\n",
    "    avg_gradient_time           = np.mean(gradient_times) if gradient_times else float('nan')\n",
    "    avg_reconfiguration_time    = np.mean(reconfiguration_times) if reconfiguration_times else float('nan')\n",
    "    avg_epoch_time              = np.mean(epoch_times) if epoch_times else float('nan')\n",
    "    avg_update_time             = np.mean(update_times) if update_times else float('nan')\n",
    "\n",
    "    print(\"\\nTiming Summary (Average per epoch):\")\n",
    "    print(f\"{'Step':<30}{'Average Time (s)':<20}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    print(f\"{'Sampling':<30}{avg_sampling_time:<20.3e}\")\n",
    "    print(f\"{'Energy Evaluation':<30}{avg_evaluation_time:<20.3e}\")\n",
    "    print(f\"{'Gradient Computation':<30}{avg_gradient_time:<20.3e}\")\n",
    "    print(f\"{'Stochastic Reconfiguration':<30}{avg_reconfiguration_time:<20.3e}\")\n",
    "    print(f\"{'Parameter Update':<30}{avg_update_time:<20.3e}\")\n",
    "    print(f\"{'Total Epoch':<30}{avg_epoch_time:<20.3e}\")\n",
    "\n",
    "    # --- Plotting ---\n",
    "    plt.figure(1)               # Create a new figure\n",
    "    plt.plot(np.real(history))  # Plot real part of energy history\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Energy\")\n",
    "    plt.title(\"Training Progress (Mean Energy)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nTraining did not complete any epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "\n",
    "sr_options = SR.SRParams(\n",
    "    min_sr          = False,\n",
    "    maxiter         = 500,\n",
    "    tol             = 1e-8,\n",
    "    reg             = 1.0,\n",
    "    # Solver parameters\n",
    "    solver_form_s   = False,\n",
    ")\n",
    "\n",
    "solver_fun  = solver_linalg_func\n",
    "precond_fun = precond_apply\n",
    "print(solver_fun)\n",
    "print(precond_fun)\n",
    "\n",
    "# functions\n",
    "ansatz      = nqs.ansatz\n",
    "loc_energy  = nqs.local_energy\n",
    "flat_grad   = nqs.flat_grad\n",
    "batch_size  = 5\n",
    "print(ansatz)\n",
    "print(loc_energy)\n",
    "print(flat_grad)\n",
    "\n",
    "# @jax.jit\n",
    "\n",
    "def single_step_train(i: int):\n",
    "    #! Generate samples\n",
    "    time_start  = time.time()\n",
    "    (states, logprobas), (configs, configs_ansatze), probabilities = nqs.sample(reset=False)\n",
    "    time_sample = time.time() - time_start\n",
    "\n",
    "    #! Single step in the NQS\n",
    "    time_start  = time.time()\n",
    "    dpar, step_info, (shapes, sizes, iscpx) = nqs.single_step_jax(\n",
    "        params              = nqs.get_params(),\n",
    "        configs             = configs,\n",
    "        configs_ansatze     = configs_ansatze,\n",
    "        probabilities       = probabilities,\n",
    "        apply_fn            = ansatz,\n",
    "        local_energy_fun    = loc_energy,\n",
    "        flat_grad_fun       = flat_grad,\n",
    "        #! Stochastic reconfiguration\n",
    "        use_sr              = True,\n",
    "        sr_options          = sr_options,\n",
    "        sr_precond_apply_fun= precond_fun,\n",
    "        sr_solve_linalg_fun = solver_fun,\n",
    "        batch_size          = batch_size,\n",
    "    )\n",
    "    time_single_step        = time.time() - time_start\n",
    "    \n",
    "    if step_info.failed:\n",
    "        return None, (None, time_sample, time_single_step)\n",
    "    \n",
    "    #! Update the parameters\n",
    "    time_start  = time.time()\n",
    "    nqs.update_parameters(dpar, -lr, shapes, sizes, iscpx)\n",
    "    time_update = time.time() - time_start\n",
    "    \n",
    "    #! Return the mean energy\n",
    "    return step_info.mean_energy, (time_update, time_sample, time_single_step)\n",
    "\n",
    "def train_function():\n",
    "    energies = []\n",
    "    for n in range(n_epo):\n",
    "        \n",
    "        time_start  = time.time() # Time the whole epoch\n",
    "        mean_energy, (time_update, time_sample, time_single_step) = single_step_train(n)\n",
    "        time_end    = time.time() - time_start\n",
    "        \n",
    "        if mean_energy is not None:\n",
    "            energies.append(mean_energy)\n",
    "            print(f\"Epoch {n + 1}/{n_epo} - Step Info: {mean_energy}, t_up:{time_update:.3e}, t_s:{time_sample:.3e}, t_ss:{time_single_step:.3e}, t_epoch:{time_end:.3e}\")\n",
    "        else:\n",
    "            print(f\"Epoch {n + 1}/{n_epo} - Step failed, stopping training.\")\n",
    "            break\n",
    "    return energies\n",
    "\n",
    "# %lprun -f train_function energies = train_function()\n",
    "energies = train_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_0 = None\n",
    "if ham.ns <= 12:\n",
    "    ham.build(verbose=True, use_numpy=True)\n",
    "    ham.diagonalize(verbose=True)\n",
    "    en_0 = ham.get_eigval()[0]\n",
    "\n",
    "n_take = 50\n",
    "if energies:\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "    print(f\"Completed {len(energies)} epochs.\")\n",
    "    print(f\"Final Mean Energy: {energies[-1]:.6f}\")\n",
    "\n",
    "    mean_e = np.mean(energies[-n_take:])\n",
    "    if en_0 is not None:\n",
    "        print(f\"Exact Energy: {en_0:.6f}\")\n",
    "        print(f\"Mean Energy: {mean_e:.6f}\")\n",
    "        rel_err = abs((mean_e - en_0) / en_0)\n",
    "        print(f\"Relative Error: {rel_err:.6f}\")\n",
    "    else:\n",
    "        print(f\"Mean Energy: {mean_e:.6f}\")\n",
    "    # Calculate averages only on successful steps\n",
    "    # --- Plotting ---\n",
    "    plt.figure(1)               # Create a new figure\n",
    "    plt.plot(np.real(energies))  # Plot real part of energy history\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(f\"Mean Energy\")\n",
    "    plt.title(\"Training Progress (Mean Energy)\")\n",
    "    plt.grid(True)\n",
    "    plt.axhline(y=en_0, color='r', linestyle='--', label='Exact Energy')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nTraining did not complete any epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.real(history))\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(ham.eig_val)\n",
    "ham.eig_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the ansatz again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 5 -n 5 \n",
    "ansatz, shape = nqsmodule.test_net_ansatz(nqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params      = net.get_params()\n",
    "\n",
    "# k0 = params['params']['Dense_0']['kernel']\n",
    "# b0 = params['params']['Dense_0']['bias']\n",
    "# k1 = params['params']['Dense_1']['kernel']\n",
    "# b1 = params['params']['Dense_1']['bias']\n",
    "# print(\"dense 0:\", (k0.shape, b0.shape))\n",
    "# print(\"dense 1:\", (k1.shape, b1.shape))\n",
    "# params['params']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Python.QES.general_python.ml.net_impl.networks.net_simple_flax as simple_flax\n",
    "simple_flax.example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate through solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -r 2 -n 2\n",
    "# apply through the solver with the states\n",
    "(configs, ansatze), probabilities, (v, means, stds) = nqs.evaluate_fun(batch_size = 1)\n",
    "means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether one can calculate the gradient for the energies directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "def loss_fn(params):\n",
    "    # recompute v with the given parameters\n",
    "    (configs, ansatze), probabilities, (v, means, stds) = nqs.evaluate_fun(batch_size=10)\n",
    "    return means\n",
    "params      = net.get_params()\n",
    "grad_loss   = jax.grad(loss_fn, holomorphic=True)(params)\n",
    "print(grad_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether one can calculate the gradient for the ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply through the solver with the states\n",
    "(configs, ansatze), probabilities, (v, means, stds) = nqs.evaluate_fun(batch_size = 10)\n",
    "means, stds, configs.shape, ansatze.shape, probabilities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_python.ml.net_impl.utils.net_utils as net_utils\n",
    "import jax \n",
    "# create the batches\n",
    "sb              = net_utils.jaxpy.create_batches_jax(configs, batch_size = 3)\n",
    "print(sb.shape)\n",
    "apply_f, params = net.get_apply(use_jax=True)\n",
    "print(apply_f(params, sb[0]).shape)\n",
    "\n",
    "# compute the gradients using JAX's vmap and scan\n",
    "# use the provided flat_grad_fun to compute the gradients\n",
    "# this is a function that computes the gradients of the network\n",
    "def scan_fun(c, x):\n",
    "    return c, jax.vmap(lambda y: net_utils.jaxpy.flat_gradient_holo_numerical_jax(apply_f, params, y), in_axes=(0,))(x)\n",
    "\n",
    "# use jax's scan to compute the gradients of the logarithmic wave function\n",
    "g = jax.lax.scan(scan_fun, None, sb)[1]\n",
    "from jax import tree_util\n",
    "\n",
    "g = tree_util.tree_map(lambda x: x.reshape((-1,) + x.shape[2:]), g)\n",
    "g = tree_util.tree_map(lambda x: x[:configs.shape[0]], g)\n",
    "\n",
    "# # g = jax.tree_map(lambda x: x / jax.lax.norm(x), g)\n",
    "g.shape\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Through the solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through the solver\n",
    "g = nqs.gradient(configs, batch_size=3)\n",
    "g.shape\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out standard gradient vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples     = v.shape[0]\n",
    "e_centered  = (v - means)\n",
    "\n",
    "# derivatives centered\n",
    "g_means     = jnp.mean(g, axis = 0)\n",
    "g_means.shape, e_centered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_centered  = g - g_means\n",
    "g_centered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get F\n",
    "F = jnp.matmul(jnp.conj(g_centered).T, e_centered) / samples\n",
    "F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nqs.update_parameters(F)\n",
    "net.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out Stochastic Reconfiguration and gradient parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out stochastic reconfiguration\n",
    "samples = v.shape[0]\n",
    "v.shape, g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_centered  = (v - means)\n",
    "\n",
    "# derivatives centered\n",
    "g_means     = jnp.mean(g, axis = 0)\n",
    "g_means.shape, e_centered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_centered  = g - g_means\n",
    "g_centered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_centered_c_t = jnp.conj(g_centered).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get F\n",
    "F = jnp.matmul(g_centered_c_t, e_centered)\n",
    "F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = jnp.matmul(g_centered_c_t, g_centered) / samples\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use standard solver\n",
    "dF = jnp.linalg.pinv(S, 1e-5) * F\n",
    "dF.shape\n",
    "dF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the stochastic reconfiguration class (with solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the solver\n",
    "import general_python.algebra.solver as solver_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### a) Without MINSR - solver = direct - no covariance S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_python.algebra.solvers import stochastic_rcnfg as sr\n",
    "\n",
    "# get the samples - apply through the solver with the states\n",
    "(configs, ansatze), probabilities, (v, means, stds) = nqs.evaluate_fun(batch_size = 10)\n",
    "means, stds, configs.shape, ansatze.shape, probabilities.shape\n",
    "\n",
    "# get the derivatives - through the solver\n",
    "g = nqs.gradient(configs, batch_size=3)\n",
    "g.shape\n",
    "\n",
    "# create the sr\n",
    "stochastic_reconfiguration = sr.StochasticReconfiguration(None, backend='jax')\n",
    "stochastic_reconfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_reconfiguration.set_values(loss  =   v, \n",
    "                                derivatives =   g,\n",
    "                                mean_loss   =   means,\n",
    "                                mean_deriv  =   None,\n",
    "                                calculate_s =   False, \n",
    "                                use_minsr   =   False)\n",
    "\n",
    "f = stochastic_reconfiguration.forces\n",
    "s = stochastic_reconfiguration.covariance_matrix\n",
    "\n",
    "f.shape, s.shape\n",
    "f, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_reconfiguration.covariance_matrix, stochastic_reconfiguration.forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = stochastic_reconfiguration.solve(use_s = True, use_minsr = False)\n",
    "solution.shape\n",
    "solution.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b) With MINSR - solver = direct - no covariance S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_python.algebra.solvers import stochastic_rcnfg as sr\n",
    "\n",
    "# get the samples - apply through the solver with the states\n",
    "(configs, ansatze), probabilities, (v, means, stds) = nqs.evaluate_fun(batch_size = 10)\n",
    "means, stds, configs.shape, ansatze.shape, probabilities.shape\n",
    "\n",
    "# get the derivatives - through the solver\n",
    "g = nqs.gradient(configs, batch_size=3)\n",
    "g.shape\n",
    "\n",
    "# create the sr\n",
    "stochastic_reconfiguration = sr.StochasticReconfiguration(None, backend='jax')\n",
    "stochastic_reconfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_reconfiguration.set_values(loss  =   v, \n",
    "                                derivatives =   g,\n",
    "                                mean_loss   =   means,\n",
    "                                mean_deriv  =   None,\n",
    "                                calculate_s =   False, \n",
    "                                use_minsr   =   True)\n",
    "\n",
    "f = stochastic_reconfiguration.forces\n",
    "s = stochastic_reconfiguration.covariance_matrix\n",
    "\n",
    "f.shape, s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nqs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = nqs.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham.build(use_numpy=True)\n",
    "ham.diagonalize()\n",
    "ham.eig_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(energies)\n",
    "# plt.axhline(energies[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
