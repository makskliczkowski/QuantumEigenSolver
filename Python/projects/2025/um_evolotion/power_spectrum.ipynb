{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1724076f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_08_2025_15-29_53 [INFO] Log file created: ./log/QES_01_08_2025_15-29_53.log\n",
      "01_08_2025_15-29_53 [INFO] Log level set to: info\n",
      "01_08_2025_15-29_53 [INFO] ############Global logger initialized.############\n",
      "01_08_2025_15-29_53 [INFO] JAX is not available. Using NumPy as the active backend.\n",
      "01_08_2025_15-29_53 [INFO] **************************************************\n",
      "01_08_2025_15-29_53 [INFO] Backend Configuration:\n",
      "01_08_2025_15-29_53 [INFO] \t\tNumPy Version: 1.26.4\n",
      "01_08_2025_15-29_53 [INFO] \t\tSciPy Version: 1.13.1\n",
      "01_08_2025_15-29_53 [INFO] \t\tJAX Version: Not Available\n",
      "01_08_2025_15-29_53 [INFO] \t\tActive Backend: numpy\n",
      "01_08_2025_15-29_53 [INFO] \t\t\tJAX Available: False\n",
      "01_08_2025_15-29_53 [INFO] \t\t\tDefault Seed: 42\n",
      "01_08_2025_15-29_53 [INFO] \t\tNumPy Backend Details:\n",
      "01_08_2025_15-29_53 [INFO] \t\t\t\tMain Module: numpy\n",
      "01_08_2025_15-29_53 [INFO] \t\t\t\tRandom Module: Generator\n",
      "01_08_2025_15-29_53 [INFO] \t\t\t\tSciPy Module: scipy\n",
      "01_08_2025_15-29_53 [INFO] \t\tActive Data Types:\n",
      "01_08_2025_15-29_53 [INFO] \t\t\t\tInteger Type: int64\n",
      "01_08_2025_15-29_53 [INFO] \t\t\t\tFloat Type: float64\n",
      "01_08_2025_15-29_53 [INFO] \t\t\t\tComplex Type: complex128\n",
      "01_08_2025_15-29_53 [INFO] \t\tHardware & Device Detection:\n",
      "01_08_2025_15-29_53 [INFO] \t\t\tCPU Cores: 48\n",
      "01_08_2025_15-29_53 [INFO] \t\tJAX Platform: Not Applicable\n",
      "01_08_2025_15-29_53 [INFO] \t\tJAX Devices Found: Not Applicable\n",
      "01_08_2025_15-29_53 [INFO] **************************************************\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['QES_BACKEND'] = 'numpy'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from typing import List, Tuple, Sequence, Dict, Any, Union\n",
    "\n",
    "# Add the parent directory (project root) to sys.path\n",
    "script_dir                  = os.path.dirname(os.curdir)\n",
    "parent_dir                  = os.path.abspath(os.path.join(script_dir, '..', '..'))\n",
    "parent_parent_dir           = os.path.abspath(os.path.join(parent_dir, '..'))\n",
    "parent_parent_parent_dir    = os.path.abspath(os.path.join(parent_parent_dir, '..'))\n",
    "\n",
    "for path in [parent_dir, parent_parent_dir, parent_parent_parent_dir]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#! General\n",
    "from QES.general_python.common.binary import get_global_logger, Array\n",
    "from QES.general_python.common import Plotter, colorsCycle, linestylesCycle\n",
    "from QES.general_python.common import Directories\n",
    "from QES.general_python.maths.math_utils import Fitter, FitterParams\n",
    "from QES.general_python.maths.statistics import Fraction\n",
    "# ------------------------------------------------------------------\n",
    "from QES.general_python.common.hdf5man import HDF5Manager\n",
    "from QES.general_python.physics import entropy, density_matrix\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "from QES.Algebra.Model.Interacting.Spin.ultrametric import UltrametricModel\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "from QES.Algebra.Properties import time_evo\n",
    "from QES.Algebra.Properties import statistical\n",
    "from QES.general_python.algebra.linalg import act, overlap, overlap_diag\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "import QES.Algebra.Operator.operators_spin as op_spin\n",
    "\n",
    "logger      = get_global_logger()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "data_dir = Directories(os.curdir, '../../2025_um_evolotion/data_add')\n",
    "data_dir = Directories(os.curdir, '../../2025_um_evolotion/data_big_times')\n",
    "# data_dir.mkdir()\n",
    "\n",
    "#! OLD\n",
    "from QES.general_python.physics.__operators__ import Operators\n",
    "\n",
    "directory_lips_random_models    = Directories(r\"/media/klimak/ObiOne/FADING_RAN_MODELS/FINAL/DATA/ETH_MAT_TIME_EVO\")\n",
    "directory_lips_random_models2   = Directories(r\"/media/klimak/ObiOne/FADING_RAN_MODELS/FINAL/DATA/ETH_MAT_TIME_EVO/ETH_MAT_TIME_EVO\")\n",
    "directory_bem2_random_models    = Directories(r\"/media/klimak/ObiOne/ObiTwoBackup/BEM_2_NEW/DATA/ETH_MAT_TIME_EVO\")\n",
    "directory_bem2_random_models2   = Directories(r\"/media/klimak/ObiOne/ObiTwoBackup/BEM_2_NEW/DATA/ETH_MAT_TIME_EVO/ETH_MAT_TIME_EVO\")\n",
    "directory_bem2_uniform_models   = Directories(r\"/media/klimak/ObiOne/FADING_RAN_MODELS/FINAL/UNIFORM/ETH_MAT_TIME_EVO_UNIFORM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46151b1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e13e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng                 = np.random.default_rng()\n",
    "rand_num            = rng.integers(0, int(1e5))\n",
    "\n",
    "bw_df               = pd.read_csv(\"model/bw.csv\", index_col=0, header=None, dtype=float)\n",
    "mls_df              = pd.read_csv(\"model/mls.csv\", index_col=0, header=None, dtype=float)\n",
    "bw_df.index         = [f'{x:.2f}' for x in bw_df.index]\n",
    "mls_df.index        = [f'{x:.2f}' for x in mls_df.index]\n",
    "bw_df.columns       = list(range(7, 17))\n",
    "mls_df.columns      = list(range(7, 16))\n",
    "\n",
    "for i in range(len(bw_df.columns)):\n",
    "    mls_df.iloc[i].plot(label=f'{bw_df.index[i]}', linestyle=next(linestylesCycle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae9e6f",
   "metadata": {},
   "source": [
    "# Results for power-spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f6ca7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from read_data import (read_energy, read_histogram, read_time_evolution_uniform, read_diagonal_matrix_elements,\n",
    "                    extract_k_functions, extract_k_times_o2, \n",
    "                    TimeEvolutionData, EnergyData, HistogramData, ObservableData, DiagonalMatrixData)\n",
    "\n",
    "# # create a colormap for the system sizes\n",
    "# colors_map, norm, cmap = Plotter.get_colormap(np.arange(8, 17, dtype = int), 'viridis_r')\n",
    "\n",
    "def get_results_single(directories_python   : List[Directories],\n",
    "                    alpha                   : float,\n",
    "                    ns                      : int,\n",
    "                    n                       : int,\n",
    "                    uniform                 : bool = False,\n",
    "                    operator                : str  = 'Sz/0'):\n",
    "    \n",
    "    logger.info(f\"alpha: {alpha:.2f}, ns: {ns}, n: {n}, uniform: {uniform}, operator: {operator}\", color='blue')\n",
    "    # --------------------------------------------------------\n",
    "    #! energy\n",
    "    # --------------------------------------------------------\n",
    "    data_energy = read_energy(directories   = directories_python,\n",
    "                            alpha           = alpha,\n",
    "                            ns              = ns,\n",
    "                            n               = n)\n",
    "    if data_energy is None:\n",
    "        logger.error(f\"Energy data not found for alpha: {alpha:.3f}, ns: {ns}, n: {n}\", color = 'red', lvl = 2)\n",
    "        return None\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    #! spectral functions\n",
    "    # --------------------------------------------------------\n",
    "    data_histogram = read_histogram(directories     = directories_python,\n",
    "                                    alpha           = alpha,\n",
    "                                    ns              = ns,\n",
    "                                    n               = n,\n",
    "                                    operator        = operator)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #! time evolution\n",
    "    # --------------------------------------------------------\n",
    "    data_evo = read_time_evolution_uniform(directories      = directories_python,\n",
    "                                        alpha               = alpha,\n",
    "                                        ns                  = ns,\n",
    "                                        n                   = n,\n",
    "                                        operator            = operator)\n",
    "    data_evo_signs = data_evo.evolution[:, 0] / np.abs(data_evo.evolution[:, 0]) / 2\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #! Diagonal matrix elements\n",
    "    # --------------------------------------------------------\n",
    "    \n",
    "    # get the indices closest to initial energies of the initial state\n",
    "    try:\n",
    "        mean_en_indices = np.argmin(np.abs(data_energy.energy.eigenvalues - data_evo.initial_state_e[:, np.newaxis]), axis=1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error finding mean energy indices: {e}\", color='red', lvl= 1)\n",
    "        mean_en_indices = None\n",
    "\n",
    "    data_diag = read_diagonal_matrix_elements(\n",
    "        directories     = directories_python,\n",
    "        alpha           = alpha,\n",
    "        ns              = ns,\n",
    "        n               = n,\n",
    "        operator        = operator,\n",
    "        mean_en_indices = mean_en_indices\n",
    "    )\n",
    "    \n",
    "    if len(data_diag.microcanonical) == data_evo.diag_ens.shape[0]:\n",
    "        q_de_me = np.mean(np.abs(data_diag.microcanonical - data_evo.diag_ens))\n",
    "    else:\n",
    "        logger.error(f\"Error calculating microcanonical diagonal ensemble: {e}\", color='red', lvl= 1)\n",
    "        q_de_me = np.nan\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #! k-functions\n",
    "    # --------------------------------------------------------\n",
    "    k_functions, k_counts = extract_k_functions(data_energy.ldos, data_energy.energy.eigenvalues, data_histogram.histogram.b)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #! multiply k and histograms together\n",
    "    # --------------------------------------------------------\n",
    "    k_times_hist, k_times_hist_norm = extract_k_times_o2(k_functions, data_histogram.histogram.v, data_histogram.histogram.b)\n",
    "\n",
    "    #! -------------------------------------------------------\n",
    "    #! create the output data\n",
    "    #! -------------------------------------------------------\n",
    "    try:\n",
    "        data_out = {\n",
    "            'n_realizations'    : len(data_energy.energy.eigenvalues),              # number of realizations\n",
    "            #! k functions for initial state\n",
    "            'k/base'            : k_functions,\n",
    "            'k/counts'          : k_counts,                                         # counts for the k-functions\n",
    "            #! multiplication of k-functions and histogram\n",
    "            'kO2/av'            : k_times_hist,                                     # k-functions multiplied by the spectral function - unnormalized          \n",
    "            'kO2/integral'      : k_times_hist_norm,                                # normalization factor for the k-functions multiplied by the spectral function\n",
    "            'kO2/omegas'        : data_histogram.histogram.b,                       # bins for the k-functions multiplied by the spectral function              \n",
    "            #! only the spectral function\n",
    "            'O2/av/raw'         : data_histogram.histogram.v,                       # average spectral function - unnormalized\n",
    "            'O2/av/smooth'      : data_histogram.cdf[1],                            # smoothed spectral function for the average spectral function\n",
    "            'O2/av/cdf'         : data_histogram.cdf[2],                            # cumulative distribution function for the average spectral function\n",
    "            'O2/av/gamma'       : data_histogram.cdf[-1],                           # Thouless energy for the average spectral function\n",
    "            # typical\n",
    "            'O2/typ/raw'        : data_histogram.histogram_typ.v,                   # typical spectral function - unnormalized\n",
    "            'O2/typ/smooth'     : data_histogram.cdf_typ[1],                        # smoothed spectral function for the typical spectral function\n",
    "            'O2/typ/cdf'        : data_histogram.cdf_typ[2],                        # cumulative distribution function for the typical spectral function\n",
    "            'O2/typ/gamma'      : data_histogram.cdf_typ[-1],                       # Thouless energy for the typical spectral function\n",
    "            'O2/count'          : data_histogram.histogram.c,                       # counts for the histogram\n",
    "            'O2/omegas'         : data_histogram.cdf[0],                            # bins for the spectral function\n",
    "            #! time evolution\n",
    "            'evo/time'          : data_evo.time,                                    # time for the time evolution\n",
    "            'evo/de'            : data_evo.diag_ens,                                # diagonal ensemble\n",
    "            'evo/me'            : data_diag.microcanonical,                         # microcanonical ensemble\n",
    "            'evo/me_q'          : q_de_me,                                          # microcanonical ensemble quality\n",
    "            'evo/av'            : np.mean(data_evo.evolution, axis = 0),            # time evolution average - probably not needed\n",
    "            'evo/corr'          : np.mean(data_evo.evolution * data_evo_signs[:, np.newaxis], axis = 0),\n",
    "            'evo/moving_av_rm'  : np.mean(data_evo.evolution_rm, axis = 0),         # time evolution average with moving average - probably not needed\n",
    "            'evo/corr_rm'       : np.mean(data_evo.evolution_rm * data_evo_signs[:, np.newaxis], axis = 0),\n",
    "            # distributions\n",
    "            'evo/dist/de/bins'  : data_evo.distribution[0],                         # bins for the distribution of the diagonal ensemble             \n",
    "            'evo/dist/de/vals'  : data_evo.distribution[1],                         # values for the distribution of the diagonal ensemble                     \n",
    "            'evo/dist/mv/bins'  : data_evo.distribution_rm[0],                      # bins for the distribution of the moving average                    \n",
    "            'evo/dist/mv/vals'  : data_evo.distribution_rm[1],                      # values for the distribution of the moving average       \n",
    "            'evo/dist/de/mom'   : np.array(data_evo.moments),                       # moments of the diagonal ensemble (E, E^2, E^3, E^4)\n",
    "            'evo/dist/mv/mom'   : np.array(data_evo.moments_rm),                    # moments of the moving average (E, E^2, E^3, E^4)\n",
    "            #! fft\n",
    "            'fft/s'             : data_evo.fft,                                     # FFT of the time evolution average - normalized and averaged\n",
    "            'fft/integral'      : data_evo.fft_n,                                   # normalization factor for the FFT of the time evolution average                        \n",
    "            'fft/om'            : data_evo.fft_om,                                  # frequencies for the FFT of the time evolution average    \n",
    "            #! statistical properties \n",
    "            'stat/gamma_th'     : data_histogram.cdf[-1],                           # is the Thouless energy\n",
    "            'stat/gamma_th_typ' : data_histogram.cdf_typ[-1],                       # is the Thouless energy for the typical spectral function\n",
    "            'stat/mean_lvl'     : data_energy.mean_level_spacing[0],                # mean level spacing for the whole spectrum\n",
    "            'stat/mean_lvl_mid' : data_energy.mean_level_spacing[1],                # mean level spacing for the middle of the spectrum\n",
    "            'stat/bw'           : data_energy.energy.bandwidth,                     # bandwidth of the energy spectrum\n",
    "            'stat/var'          : data_energy.energy.sigma_2_e,                     # variance of the energy spectrum\n",
    "            'stat/gap_ratio'    : data_energy.gap_ratios[0],\n",
    "            'stat/gap_ratio_mid': data_energy.gap_ratios[1],                        # gap ratio for the energy distribution\n",
    "            #! entropies\n",
    "            'entropy/vn'        : data_energy.entropy.von_neumann,                  # von Neumann entropy for the initial state\n",
    "            'entropy/tsallis'   : data_energy.entropy.tsallis,                      # Tsallis entropy for the initial state\n",
    "            'entropy/schmidt'   : data_energy.entropy.schmidt_gap,                  # Schmidt gap for the initial state\n",
    "            #! iprs of the initial state\n",
    "            'ipr/mean/0.5'      : np.mean(data_energy.iprs[0.5], axis = 0),                     # iprs for the initial state\n",
    "            'ipr/mean/1.0'      : np.mean(data_energy.iprs[1.0], axis = 0),                     # iprs for the initial state\n",
    "            'ipr/mean/2.0'      : np.mean(data_energy.iprs[2.0], axis = 0),                     # iprs for the initial state\n",
    "            'ipr/mean/3.0'      : np.mean(data_energy.iprs[3.0], axis = 0),                     # iprs for the initial state\n",
    "            'ipr/mean/4.0'      : np.mean(data_energy.iprs[4.0], axis = 0),                     # iprs for the initial state\n",
    "            'ipr/raw/0.5'       : data_energy.iprs[0.5],                                        # iprs for the initial state\n",
    "            'ipr/raw/1.0'       : data_energy.iprs[1.0],                                        # iprs for the initial state\n",
    "            'ipr/raw/2.0'       : data_energy.iprs[2.0],                                        # iprs for the initial state\n",
    "            'ipr/raw/3.0'       : data_energy.iprs[3.0],                                        # iprs for the initial state\n",
    "            'ipr/raw/4.0'       : data_energy.iprs[4.0],                                        # iprs for the initial state\n",
    "            #! participation entropies of the initial state\n",
    "            'part_ent/0.5'      : np.mean(data_energy.participation_entropies[0.5]),\n",
    "            'part_ent/1.0'      : np.mean(data_energy.participation_entropies[1.0]),\n",
    "            'part_ent/2.0'      : np.mean(data_energy.participation_entropies[2.0]),\n",
    "            'part_ent/3.0'      : np.mean(data_energy.participation_entropies[3.0]),\n",
    "            'part_ent/4.0'      : np.mean(data_energy.participation_entropies[4.0]),\n",
    "            'part_ent/raw/0.5'  : data_energy.participation_entropies[0.5],\n",
    "            'part_ent/raw/1.0'  : data_energy.participation_entropies[1.0],\n",
    "            'part_ent/raw/2.0'  : data_energy.participation_entropies[2.0],\n",
    "            'part_ent/raw/3.0'  : data_energy.participation_entropies[3.0],\n",
    "            'part_ent/raw/4.0'  : data_energy.participation_entropies[4.0],\n",
    "            #! diagonal matrix element properties\n",
    "            'diag/mean'         : data_diag.diagonal[1],                                    # mean of the diagonal matrix elements\n",
    "            'diag/variance'     : data_diag.diagonal[2],                                    # variance of the diagonal\n",
    "            'diag/hist/bins'    : data_diag.histogram[1],\n",
    "            'diag/hist/values'  : data_diag.histogram[0],                                    # histogram of the diagonal matrix elements\n",
    "            'diag/stat/gauss'   : data_diag.gaussianity,\n",
    "            'diag/stat/binder'  : data_diag.binder_cumulant,\n",
    "            'diag/stat/micro'   : data_diag.microcanonical,                             # microcanonical ensemble for the diagonal matrix elements\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating output data: {e}\", color='red', lvl= 1)\n",
    "        return None\n",
    "    logger.info(f\"Extracted data for alpha: {alpha:.2f}, ns: {ns}, n: {n}\", color='purple')\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "277a2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas          = np.arange(0.2, 1.0, 0.02)\n",
    "sites           = np.arange(7, 17, dtype=int)\n",
    "n               = 1\n",
    "uniform         = True\n",
    "uniform_text    = 'uniform' if uniform else 'log'\n",
    "\n",
    "directory_s     = Directories(os.curdir, 'saved', 'uniform', 'processed')\n",
    "directory_s.mkdir(parents=True, exist_ok=True)\n",
    "for alpha in alphas:\n",
    "    for ns in sites:\n",
    "        directories_python      = [ Directories(os.path.abspath(f'/home/klimak/Codes/QuantumEigenSolver/Python/projects/2025/um_evolotion/data_big_times/{uniform_text}/')),\n",
    "                                    Directories(os.path.abspath(f'/home/klimak/Codes/QuantumEigenSolver/Python/projects/2025_um_evolotion/data_big_times/{uniform_text}/')),\n",
    "                                    Directories(os.path.abspath(f'/media/klimak/ObiOne/Fading_random_models/data_big_times/{uniform_text}/')),\n",
    "                                    Directories(os.path.abspath('data_final'), 'uniform' if uniform else 'log')]\n",
    "        try:\n",
    "            y                   = get_results_single(directories_python     =   directories_python,\n",
    "                                                        alpha               =   alpha,\n",
    "                                                        ns                  =   ns,\n",
    "                                                        n                   =   n,\n",
    "                                                        uniform             =   True)\n",
    "            if y is None:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing alpha {alpha:.2f}, ns {ns}, n {n}: {e}\", color='red')\n",
    "            continue\n",
    "        HDF5Manager.save_data_to_file(directory=directory_s, filename = f'alpha_{alpha:.2f}_ns_{ns}_n_{n}.h5', data_to_save=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff35a25",
   "metadata": {},
   "source": [
    "### Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1192592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_and_stack(xs         : Sequence[np.ndarray],\n",
    "                    ys         : Sequence[np.ndarray],\n",
    "                    fill_value : float      = np.nan,\n",
    "                    shared_x   : np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Align multiple (x, y) samples onto their union x-axis and stack\n",
    "    the y-arrays, filling gaps with fill_value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xs\n",
    "        Sequence of 1D arrays of x-coordinates.\n",
    "    ys\n",
    "        Sequence of 1D arrays of y-values; must have same length as xs.\n",
    "    fill_value\n",
    "        Scalar to use where a sample has no y at a given union-x.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_union : np.ndarray, shape (F,)\n",
    "        Sorted unique union of all x-values.\n",
    "    Y       : np.ndarray, shape (N, F)\n",
    "        Stacked y-values for N samples, aligned to x_union.\n",
    "    \"\"\"\n",
    "    if shared_x is None:\n",
    "        if len(xs) != len(ys):\n",
    "            raise ValueError(\"`xs` and `ys` must have the same number of samples\")\n",
    "    else:\n",
    "        logger.info(\"Using shared x-axis, it gives us indices in samples in y for a given x\", lvl=1)\n",
    "        logger.info(f\"Shared x-axis: {shared_x}\", lvl=2)\n",
    "        \n",
    "    # compute sorted union of all x\n",
    "    x_union     = np.unique(np.concatenate(xs))\n",
    "    print(f\"Union x-axis: {x_union}\")\n",
    "    n_samples   = len(ys)\n",
    "    n_columns   = x_union.size\n",
    "\n",
    "    # allocate output array\n",
    "    Y           = np.full((n_samples, n_columns), fill_value, dtype=ys[0].dtype if hasattr(ys[0], 'dtype') else float)\n",
    "    print(f\"Y shape: {Y.shape}\")\n",
    "\n",
    "    # fill each row by searching where its x_i lands in x_union\n",
    "    idx_in_x    = 0\n",
    "    for i in range(n_samples):\n",
    "        print(f\"\\tSample {i}\")\n",
    "        if shared_x is not None:\n",
    "            # move to the next sample in y\n",
    "            if i >= shared_x[idx_in_x]:\n",
    "                idx_in_x    +=   1\n",
    "            print(f\"\\t\\tidx_in_x: {idx_in_x}\")\n",
    "            x_i = xs[idx_in_x]\n",
    "            y_i = ys[i]\n",
    "        else:\n",
    "            x_i = xs[i]\n",
    "            y_i = ys[i]            \n",
    "            \n",
    "        idx         = np.searchsorted(x_union, x_i)\n",
    "        Y[i, idx]   = y_i\n",
    "\n",
    "    return x_union, Y\n",
    "\n",
    "x1 = [0, 1, 2, 3, 4]\n",
    "y1 = [1] * 5\n",
    "y12= [5] * 5\n",
    "x2 = [1, 2, 3, 4, 5, 6]\n",
    "y2 = [2] * 6\n",
    "y3 = [3] * 6\n",
    "\n",
    "# align_and_stack(\n",
    "#     [x1, x2],\n",
    "#     [y1, y12, y2, y3],\n",
    "#     fill_value=np.nan,\n",
    "#     shared_x=[2, 4]\n",
    "# )\n",
    "\n",
    "# def sum_previous()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b52b3",
   "metadata": {},
   "source": [
    "# Other results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34347c02",
   "metadata": {},
   "source": [
    "#### Get LDOS from the full dynamics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "430dcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def get_ldos_full_ns_scan(\n",
    "    directories      : list,\n",
    "    dir_save_parsed  : Directories,\n",
    "    dir_save_raw_new : Directories,\n",
    "    alpha            : float,\n",
    "    ns_list          : list[int],\n",
    "    n                : int   = None,\n",
    "    statetype        : str   = 'ME',\n",
    "    verbose          : bool  = False,\n",
    "    rm_and_mv_2_new  : bool  = True):\n",
    "    \"\"\"\n",
    "    Compute LDOS statistics for a fixed coupling α over a range of system sizes n_s.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directories : list[str]\n",
    "        Templates for input directories, with placeholders for n_s and α.\n",
    "    dir_save_parsed : Directories\n",
    "        Directory in which to save parsed summary data.\n",
    "    dir_save_raw_new : Directories\n",
    "        Directory in which to move raw LDOS files after processing.\n",
    "    alpha : float\n",
    "        Coupling strength α for the UltrametricModel.\n",
    "    ns_list : list[int]\n",
    "        List of system‐size exponents n_s (so Hilbert‐space dimension N_h=2**n_s).\n",
    "    ops : list\n",
    "        Operators to resolve and read (unused in this version).\n",
    "    n : int, optional\n",
    "        Maximum distance n for the model (Ultrametric cutoff).\n",
    "    statetype : str\n",
    "        Dataset key in HDF5 files (e.g. 'ME').\n",
    "    model_str : str\n",
    "        Model identifier string (e.g. 'UM').\n",
    "    verbose : bool\n",
    "        If True, print detailed logging.\n",
    "    rm_and_mv_2_new : bool\n",
    "        If True, move raw files into a new directory per n_s.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Aggregated data_out with keys\n",
    "        '{n_s}/ipr/mean/{q}', '{n_s}/ipr/raw/{q}',\n",
    "        '{n_s}/part_ent/mean/{q}', '{n_s}/part_ent/raw/{q}',\n",
    "        plus 'ns' list of processed n_s values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Confirm removal if requested\n",
    "    if rm_and_mv_2_new:\n",
    "        resp = input(f\"Are you sure you want to remove old LDOS files into {dir_save_raw_new}? 'no' to cancel: \")\n",
    "        if resp.lower() == 'no':\n",
    "            logger.error(\"Operation cancelled by user.\", 1, color='red')\n",
    "            return\n",
    "        dir_save_raw_new.mkdir(parents=True, exist_ok=True)\n",
    "    dir_save_parsed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # spectral moments q \in {0.5,1,2,3,4}\n",
    "    qs       = [0.1, 0.5, 2.0, 3.0, 4.0]\n",
    "    data_out = {}\n",
    "    # initialize containers for each n_s and q\n",
    "    for n_s in ns_list:\n",
    "        for q in qs:\n",
    "            key_mean_ipr            = f'{n_s}/ipr/mean/{q:.1f}'\n",
    "            key_raw_ipr             = f'{n_s}/ipr/raw/{q:.1f}'\n",
    "            key_mean_ent            = f'{n_s}/part_ent/mean/{q:.1f}'\n",
    "            key_raw_ent             = f'{n_s}/part_ent/raw/{q:.1f}'\n",
    "            data_out[key_mean_ipr]  = []\n",
    "            data_out[key_raw_ipr]   = 0.0\n",
    "            data_out[key_mean_ent]  = []\n",
    "            data_out[key_raw_ent]   = 0.0\n",
    "\n",
    "    ns_done = []\n",
    "    \n",
    "    # loop over system sizes\n",
    "    for i_s, n_s in enumerate(ns_list):\n",
    "        logger.title(f\"Scanning n_s = {n_s}, α = {alpha}\", 50, fill='-', lvl=3)\n",
    "        dirs_in = [d.format(n_s, alpha) for d in directories]\n",
    "        model   = UltrametricModel(ns=n_s, n=n, alphas=alpha)\n",
    "        mstr    = str(model)\n",
    "\n",
    "        if rm_and_mv_2_new:\n",
    "            raw_new_in = dir_save_raw_new / mstr\n",
    "            raw_new_in.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        logger.info(f\"Reading from: {dirs_in} : {mstr}\", lvl=4, verbose=verbose, color='green')\n",
    "        all_files   = [f for d in dirs_in for f in d.list_files(filters=[lambda x: str(x).endswith('.h5')])]\n",
    "        logger.info(f\"Found {len(all_files)} .h5 files\", lvl=4, verbose=verbose, color='green')\n",
    "\n",
    "        # categorize\n",
    "        ldos_files  = [str(f) for f in all_files if \"ldos_\" in str(f)]\n",
    "        logger.info(f\" → {len(ldos_files)} LDOS files\", lvl=4, verbose=verbose, color='green')\n",
    "\n",
    "        if not ldos_files or len(ldos_files) == 0:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # load & concatenate\n",
    "            ldos_list = HDF5Manager.load_data_from_multiple_files(file_paths = ldos_files, dataset_keys = [statetype])\n",
    "            # extract arrays, flatten, filter empty or pathological rows\n",
    "            ldos = [x[statetype] for x in ldos_list]\n",
    "            ldos = [row for arr in ldos for row in arr if row.shape[0] > 0]\n",
    "            ldos = np.array(ldos, dtype=float)\n",
    "            ldos = ldos[~np.all(ldos < 1e-14, axis=1)]\n",
    "            ldos = ldos[~np.all(ldos < -1e4 , axis=1)]\n",
    "            # compute IPR_q = ∑_i |ψ_i|^{2q}  (square=False since LDOS already squared)\n",
    "            iprs        = { q: statistical.inverse_participation_ratio(ldos.T, q=q, square=False) for q in qs }\n",
    "            # participation entropy S_q = (1 / (1 - q)) log ∑_i |ψ_i|^{2q}\n",
    "            part_ent    = { q: entropy.participation_entropy(ldos.T, q=q/2, square=False) for q in qs }\n",
    "            \n",
    "            for q in qs:\n",
    "                k_ipr_mean              = f'{n_s}/ipr/mean/{q:.1f}'\n",
    "                k_ipr_raw               = f'{n_s}/ipr/raw/{q:.1f}'\n",
    "                k_ent_mean              = f'{n_s}/part_ent/mean/{q:.1f}'\n",
    "                k_ent_raw               = f'{n_s}/part_ent/raw/{q:.1f}'\n",
    "\n",
    "                data_out[k_ipr_mean].append(np.mean(iprs[q]))\n",
    "                data_out[k_ipr_raw]     = iprs[q]\n",
    "                data_out[k_ent_mean].append(np.mean(part_ent[q]))\n",
    "                data_out[k_ent_raw]     = part_ent[q]\n",
    "\n",
    "            # move & remove raw\n",
    "            if rm_and_mv_2_new:\n",
    "                rnd     = np.random.randint(1, 99999)\n",
    "                fname   = f\"ldos_R={rnd}.h5\"\n",
    "                fpath   = raw_new_in / fname\n",
    "                HDF5Manager.save_data_to_file(\n",
    "                    directory    = raw_new_in,\n",
    "                    filename     = fname,\n",
    "                    data_to_save = {statetype: ldos},\n",
    "                    overwrite    = True\n",
    "                )\n",
    "                logger.info(f\"Saved LDOS to {fpath}\", level=4, color='green')\n",
    "\n",
    "                for f in ldos_files:\n",
    "                    if raw_new_in in pathlib.Path(f).parents:\n",
    "                        continue\n",
    "                    try:\n",
    "                        os.remove(f)\n",
    "                        logger.info(f\"Removed file {f}\", level=5, color='blue')\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Could not remove {f}: {e}\", level=5, color='red')\n",
    "\n",
    "        except Exception as exc:\n",
    "            logger.error(f\"Error for n_s={n_s}: {exc}\", lvl=1, color='red')\n",
    "            continue\n",
    "        ns_done.append(n_s)\n",
    "\n",
    "    # prune unused sizes\n",
    "    data_out        = {k: v for k, v in data_out.items() if any(str(n_s) in k for n_s in ns_done)}\n",
    "    if len(ns_done) == 0:\n",
    "        logger.error(\"No valid n_s sizes found, exiting.\", lvl=1, color='red')\n",
    "        return None\n",
    "    \n",
    "    #! fit the fractal dimension\n",
    "    if data_out:\n",
    "        data_out['qs']      = qs\n",
    "        dims                = 2**np.array(ns_done, dtype=float)\n",
    "        logdims             = np.log(dims)\n",
    "    \n",
    "        # fit to a * D**(-b), where b / (q - 1) is the d_q fractal dimension\n",
    "        try:\n",
    "            for q in qs:\n",
    "                if q == 1.0:\n",
    "                    continue\n",
    "                fitfun  = lambda x, a, b: a * x ** (-b)\n",
    "                # fitfun      = lambda x, a, b: a + b * x\n",
    "                x       = dims / dims.max()\n",
    "                iprs    = np.array([data_out[f'{x}/ipr/mean/{q:.1f}'] for x in ns_done]).flatten()\n",
    "                fitpar  = Fitter.fitAny(x, iprs, fitfun, skipF = 1)\n",
    "                # fitpar  = Fitter.fitAny(ns_done, iprs, fitfun, skipF = 3)\n",
    "                popt    = fitpar.get_popt()\n",
    "                fracd   = popt[1] / (q-1.0)\n",
    "                # logger.info(f\"Fitted fractal dimension for q={q}: {fracd}\", lvl=3, color='blue')\n",
    "                # data_out['fractal/av/{q:.1f}/equation'] = 'a * D**(-b)'\n",
    "                data_out[f'fractal/av/{q:.1f}/a']           = popt[0]\n",
    "                data_out[f'fractal/av/{q:.1f}/b']           = popt[1]\n",
    "                data_out[f'fractal/av/{q:.1f}/d_q']         = fracd\n",
    "        except Exception as e:\n",
    "            for q in qs:\n",
    "                data_out[f'fractal/av/{q:.1f}/a']           = np.nan\n",
    "                data_out[f'fractal/av/{q:.1f}/b']           = np.nan\n",
    "                data_out[f'fractal/av/{q:.1f}/d_q']         = np.nan\n",
    "                \n",
    "        # try now to fit to something else: alpha_c * D**(-b) + abs(c_{ipr_infty})\n",
    "        try:\n",
    "            for q in qs:\n",
    "                def fitfun(x, a, b, c):\n",
    "                    # return a*x**(-b) + c**2\n",
    "                    return (a/c) * (x**(-b) + 1)\n",
    "                iprs    = np.array([data_out[f'{x}/ipr/mean/{q:.1f}'] for x in ns_done]).flatten()\n",
    "                fitpar  = Fitter.fitAny(dims, iprs, fitfun, skipF=1, skipL=1)\n",
    "                popt    = fitpar.get_popt()\n",
    "                # logger.info(f\"Fitted fractal dimension for q={q}: {fracd}\", lvl=3, color='blue')\n",
    "                # data_out['fractal/av/mod/{q:.1f}/equation'] = 'a * D**(-b) + c'\n",
    "                data_out[f'fractal/av/mod/{q:.1f}/a']       = popt[0]\n",
    "                data_out[f'fractal/av/mod/{q:.1f}/b']       = popt[1]\n",
    "                data_out[f'fractal/av/mod/{q:.1f}/c']       = popt[2]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fitting modified fractal dimension for q={q}: {e}\", lvl=3, color='red')\n",
    "            for q in qs:\n",
    "                data_out[f'fractal/av/mod/{q:.1f}/b']       = np.nan\n",
    "                data_out[f'fractal/av/mod/{q:.1f}/c']       = np.nan\n",
    "        \n",
    "        # try typical fractal dimension from the participation entropy\\\n",
    "        try:\n",
    "            for q in qs:\n",
    "                if q == 1.0:\n",
    "                    logger.warning(\"Skipping q=1.0 as it leads to singularities in the fit\", lvl=4, color='yellow')\n",
    "                    continue\n",
    "                entros      = np.array([data_out[f'{x}/part_ent/mean/{q:.1f}'] for x in ns_done]).flatten()\n",
    "                entros_diff = np.diff(entros)\n",
    "                typical_f   = entros_diff / np.diff(logdims)\n",
    "                data_out[f'fractal/typ/{q:.1f}/typical']    = typical_f\n",
    "                # data_out['fractal/typ/{q:.1f}/equation']    = 'dS/dlog(D)'\n",
    "        except Exception as e:\n",
    "            data_out[f'fractal/typ/{q:.1f}/typical']        = np.nan\n",
    "    # save summary\n",
    "    data_out['ns']  = ns_done\n",
    "    \n",
    "    if data_out:\n",
    "        out_fname = f\"ldos_data_a={alpha:.3f},n={n}.h5\"\n",
    "        HDF5Manager.save_data_to_file(\n",
    "            directory    = dir_save_parsed,\n",
    "            filename     = out_fname,\n",
    "            data_to_save = data_out,\n",
    "            overwrite    = True\n",
    "        )\n",
    "    return data_out\n",
    "\n",
    "Nss     = [8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "g0      = 1.0\n",
    "alphas  = np.arange(0.3, 0.98, 0.01)\n",
    "# alphas  = [0.760, 0.80, 0.90]\n",
    "state   = 'ME'\n",
    "n       = 1  # number of grains\n",
    "\n",
    "#! get the directories\n",
    "directory_data          = Directories(r\",ultrametric,Ns={},\" + f\"N={n}\" + \",g0=1.00,alpha={:.3f}\")\n",
    "directory_data_lips     = directory_lips_random_models  / directory_data\n",
    "directory_data_bem      = directory_bem2_random_models  / directory_data\n",
    "directory_data_lips2    = directory_lips_random_models2 / directory_data\n",
    "directory_data_bem2     = directory_bem2_random_models2 / directory_data\n",
    "directory_data_uniform  = directory_bem2_uniform_models / directory_data\n",
    "dir_save_raw_new        = Directories('/media/klimak/ObiOne/FADING_RAN_MODELS/reparsed_all_together/time_evo', 'um')\n",
    "directory_data          = [directory_data_lips, directory_data_bem, directory_data_lips2, directory_data_bem2, dir_save_raw_new, directory_data_uniform]\n",
    "directory_s             = Directories(os.curdir, 'saved', 'log', 'processed')\n",
    "\n",
    "alphas_done     = []\n",
    "qs              = [0.1, 0.5, 2.0, 3.0, 4.0]\n",
    "frac_av         = { q : [] for q in qs }\n",
    "frac_av_mod     = { q : [] for q in qs }\n",
    "frac_av_mod_c   = { q : [] for q in qs }\n",
    "frac_typ        = { q : [] for q in qs }\n",
    "logger.title(\"Starting LDOS full scan\", 50, fill='-', lvl=1)\n",
    "\n",
    "for alpha in alphas:  \n",
    "    logger.title(f\"a = {alpha:.3f}\", 50, \"-\", 1)\n",
    "    data_out = get_ldos_full_ns_scan(\n",
    "        directories         = directory_data,\n",
    "        dir_save_parsed     = directory_s,\n",
    "        dir_save_raw_new    = dir_save_raw_new,\n",
    "        alpha               = alpha,\n",
    "        ns_list             = Nss,\n",
    "        n                   = n,\n",
    "        statetype           = state,\n",
    "        verbose             = True,\n",
    "        rm_and_mv_2_new     = False\n",
    "    )\n",
    "    if data_out is None:\n",
    "        logger.error(f\"No data found for alpha = {alpha:.3f}, skipping.\", lvl=1, color='red')\n",
    "        continue\n",
    "    for q in qs:\n",
    "        f_av     = data_out.get(f'fractal/av/{q:.1f}/d_q', None)\n",
    "        f_av_mod = data_out.get(f'fractal/av/mod/{q:.1f}/b', None)\n",
    "        f_av_c   = data_out.get(f'fractal/av/mod/{q:.1f}/c', None)\n",
    "        f_typ    = np.mean(data_out.get(f'fractal/typ/{q:.1f}/typical', None)) if f_av is not None else None\n",
    "        \n",
    "        # append the values\n",
    "        frac_av[q].append(f_av)\n",
    "        frac_av_mod[q].append(f_av_mod)\n",
    "        frac_av_mod_c[q].append(f_av_c)\n",
    "        frac_typ[q].append(f_typ)\n",
    "    alphas_done.append(alpha)\n",
    "    logger.info(f\"Done for alpha = {alpha:.3f}\", lvl=1, color='blue')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax     = Plotter.get_subplots(nrows=3, ncols=len(qs), figsize=(3 * len(qs), 3 * 3), dpi = 150, sharex=True, sharey=True)\n",
    "len_qs      = len(qs)\n",
    "for i, q in enumerate(qs):\n",
    "    logger.info(f\"Plotting for q={q}\", lvl=1, color='blue')\n",
    "    ax[i].set_title(f\"q = {q:.1f}\", fontsize=12)\n",
    "    ax[-i - 1].set_xlabel(r\"$\\alpha$\")\n",
    "    ax[0 * len_qs].set_ylabel(r\"$d_q$\")\n",
    "    ax[1 * len_qs].set_ylabel(r\"$\\tilde {d}_q, P^{-1}_{q, \\infty}$\")\n",
    "    ax[2 * len_qs].set_ylabel(r\"$D^{\\rm typ}_q$\")\n",
    "    \n",
    "    # plot the average fractal dimension\n",
    "    frac_av_q       = np.array(frac_av[q]) \n",
    "    logger.info(f\"Fractal dimension (average) for q={q}: {frac_av_q}\", lvl=2, color='blue')\n",
    "    frac_av_mod_q   = np.abs(np.array(frac_av_mod[q]) / (q - 1.0))\n",
    "    logger.info(f\"Fractal dimension (average mod) for q={q}: {frac_av_mod_q}\", lvl=2, color='green')\n",
    "    frac_av_mod_c_q = np.abs(frac_av_mod_c[q])\n",
    "    logger.info(f\"Fractal dimension (average mod c) for q={q}: {frac_av_mod_c_q}\", lvl=2, color='white')\n",
    "    frac_typ_q      = np.array(frac_typ[q])\n",
    "    logger.info(f\"Fractal dimension (typical) for q={q}: {frac_typ_q}\", lvl=2, color='red')\n",
    "    ax[0 * len_qs + i].scatter(alphas_done, frac_av_q, marker='o', label=r'$d_{q}$' if i == 0 else None)\n",
    "    ax[1 * len_qs + i].scatter(alphas_done, frac_av_mod_q, marker='o', label=r'$d_q^{\\rm modified }$')\n",
    "    ax[1 * len_qs + i].scatter(alphas_done, frac_av_mod_c_q, marker='s', label=r'$P^{-1}_{q, \\infty}$')\n",
    "    ax[2 * len_qs + i].scatter(alphas_done, frac_typ_q, marker='^', label=r'$D_{q}^{\\rm typ}(\\mathcal{L}^{\\max})$')\n",
    "\n",
    "    Plotter.set_legend(ax[0 * len_qs + i], loc='upper left', fontsize=8, ncol=1)\n",
    "    Plotter.set_legend(ax[1 * len_qs + i], loc='upper left', fontsize=8, ncol=1)\n",
    "    Plotter.set_legend(ax[2 * len_qs + i], loc='upper left', fontsize=8, ncol=1)\n",
    "    \n",
    "for a in ax:\n",
    "    Plotter.set_tickparams(a)\n",
    "    a.set_ylim([-.05, 2.05])\n",
    "    a.axvline(0.71, color='red', linestyle='--', label=r\"$\\alpha_c \\approx 0.71$\")\n",
    "    a.axhline(0.0, color='black', linestyle='--')\n",
    "    a.axhline(1.0, color='black', linestyle='--')\n",
    "    a.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c29ca1",
   "metadata": {},
   "source": [
    "##### Scan with $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b36d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27_06_2025_10-28_31 [INFO] \t\t\t->Reading alpha = 0.7600000000000002 - ns = 8\n",
     ]
    }
   ],
   "source": [
    "from QES.general_python.physics.__operators__ import Operators\n",
    "\n",
    "directory_lips_random_models    = Directories(r\"/media/klimak/ObiOne/FADING_RAN_MODELS/FINAL/DATA/ETH_MAT_TIME_EVO\")\n",
    "directory_lips_random_models2   = Directories(r\"/media/klimak/ObiOne/FADING_RAN_MODELS/FINAL/DATA/ETH_MAT_TIME_EVO/ETH_MAT_TIME_EVO\")\n",
    "directory_bem2_random_models    = Directories(r\"/media/klimak/ObiOne/ObiTwoBackup/BEM_2_NEW/DATA/ETH_MAT_TIME_EVO\")\n",
    "directory_bem2_random_models2   = Directories(r\"/media/klimak/ObiOne/ObiTwoBackup/BEM_2_NEW/DATA/ETH_MAT_TIME_EVO/ETH_MAT_TIME_EVO\")\n",
    "\n",
    "def get_ldos_full(directories   : list,\n",
    "                dir_save_parsed : Directories,\n",
    "                dir_save_raw_new: Directories,\n",
    "                alphas          : list, \n",
    "                ns              : int, \n",
    "                ops             : list, # list of operators to read\n",
    "                n               : int   = None, \n",
    "                statetype       : str   = 'ME', \n",
    "                model           : str   = 'UM',\n",
    "                verbose         : bool  = False,\n",
    "                rm_and_mv_2_new : bool  = True,\n",
    "                ):   \n",
    "    \n",
    "    # are you sure you want to remove?\n",
    "    if rm_and_mv_2_new:\n",
    "        # check for user input 'enter' to continue\n",
    "        out = input(f\"Are you sure you want to remove the directories as {dir_save_raw_new} is set? 'no' to cancel...\")\n",
    "        if out.lower() == 'no':\n",
    "            logger.error(\"Operation cancelled by user.\", 1, color='red')\n",
    "            return\n",
    "        else:\n",
    "            dir_save_raw_new.mkdir(parents=True, exist_ok=True) # create the directory if it does not exist\n",
    "            \n",
    "    dir_save_parsed.mkdir(parents=True, exist_ok=True)          # create the directory if it does not exist\n",
    "    \n",
    "    # go through the operators\n",
    "    # for if_op, f_op in enumerate(ops):\n",
    "    # f_op_n                  = Operators.resolve_operator(f_op, ns)  # resolve the operator name to a string\n",
    "    qs                      = [0.5, 1.0, 2.0, 3.0, 4.0] # divide by 2, as we have the LDOS, which is already squared\n",
    "    data_out                = {}\n",
    "    data_out.update({ f'{a:.3f}/ipr/mean/{q:.1f}' : [] for q in qs for a in alphas })\n",
    "    data_out.update({ f'{a:.3f}/ipr/raw/{q:.1f}' : 0.0 for q in qs for a in alphas })\n",
    "    data_out.update({ f'{a:.3f}/part_ent/mean/{q:.1f}' : [] for q in qs for a in alphas })\n",
    "    data_out.update({ f'{a:.3f}/part_ent/raw/{q:.1f}' : 0.0 for q in qs for a in alphas })\n",
    "    # estimation!\n",
    "    Nh                      = 2**ns\n",
    "    \n",
    "    # go through alphas\n",
    "    alphas_done             = []\n",
    "    for ia, alpha in enumerate(alphas):\n",
    "        logger.title(f\"Reading alpha = {alpha} - ns = {ns}\", 50, \"-\", 3)\n",
    "        dirs_in                 = [d.format(ns, alpha) for d in directories]\n",
    "        model                   = UltrametricModel(ns = ns, n = n, alphas = alpha)\n",
    "        modelstr                = str(model)\n",
    "        if rm_and_mv_2_new:\n",
    "            dir_save_raw_new_in = dir_save_raw_new / modelstr\n",
    "            dir_save_raw_new_in.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "        #######################################\n",
    "        logger.info(f\"Reading from: {dirs_in} : {modelstr}\", 4, verbose = verbose, color='green')\n",
    "        \n",
    "        filelist        = [x for d in dirs_in for x in d.list_files(filters = [lambda x: str(x).endswith('h5')])]\n",
    "        logger.info(f\"Found {len(filelist)} files...\", 4, verbose = verbose, color='green')\n",
    "        filelist_ldos   = [str(x) for x in filelist if \"ldos_\" in str(x)]\n",
    "        filelist_diag   = [str(x) for x in filelist if \"diag_\" in str(x)]\n",
    "        filelist_evo    = [str(x) for x in filelist if \"evo_\" in str(x)]\n",
    "        filelist_stat   = [str(x) for x in filelist if \"stat_\" in str(x) or \"energy_\" in str(x)]\n",
    "        logger.info(f\"Found {len(filelist_ldos)} LDOS files, {len(filelist_diag)} diagonal files, {len(filelist_evo)} time evolution files, and {len(filelist_stat)} statistics files...\", 4, verbose = verbose, color='green')\n",
    "        \n",
    "        if len(filelist) > 0:\n",
    "            try:\n",
    "                ldos        = HDF5Manager.load_data_from_multiple_files(file_paths=filelist_ldos, dataset_keys=[statetype])\n",
    "                ldos        = [x[statetype] for x in ldos]\n",
    "                ldos        = [y for x in ldos for y in x if y.shape[0] > 0]    # flatten and remove empty arrays\n",
    "                ldos        = np.array(ldos, dtype=float)\n",
    "                #! check if it's all zeros on axis 1, if so, skip this row\n",
    "                ldos        = ldos[~np.all(ldos < 1e-14, axis=1)]\n",
    "                #! check if is smaller that -1e5 - threshold for save\n",
    "                ldos        = ldos[~np.all(ldos < -1e5, axis=1)]\n",
    "                #! create iprs\n",
    "                iprs                    = { q: statistical.inverse_participation_ratio(ldos.T, q = q, square=False) for q in qs }\n",
    "                #! create participation entropies\n",
    "                participation_entropies = { q: entropy.participation_entropy(ldos.T, q = q / 2, square=False) for q in qs }\n",
    "\n",
    "                for q in qs:\n",
    "                    data_out[f'{alpha:.3f}/ipr/mean/{q:.1f}'].append(np.mean(iprs[q]))\n",
    "                    data_out[f'{alpha:.3f}/ipr/raw/{q:.1f}']        = iprs[q]\n",
    "                    data_out[f'{alpha:.3f}/part_ent/mean/{q:.1f}'].append(np.mean(participation_entropies[q]))\n",
    "                    data_out[f'{alpha:.3f}/part_ent/raw/{q:.1f}']   = participation_entropies[q]\n",
    "\n",
    "                #! create the file in the new directory and remove the old ones\n",
    "                if rm_and_mv_2_new:\n",
    "                    random_num  = np.random.randint(1, 99999)\n",
    "                    file_name   = f\"ldos_R={random_num}.h5\"\n",
    "                    file_path   = dir_save_raw_new_in / file_name\n",
    "                    HDF5Manager.save_data_to_file(directory=dir_save_raw_new_in, filename=file_name, data_to_save={statetype: ldos}, overwrite=True)\n",
    "                    logger.info(f\"Saved LDOS data to {file_path}\", 4, color='green')\n",
    "                    # remove the old files\n",
    "                    for file in filelist_ldos:\n",
    "                        \n",
    "                        #! do not remove the new directory\n",
    "                        if dir_save_raw_new_in in file:\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            os.remove(file)\n",
    "                            logger.info(f\"Removed file {file}\", 5, color='blue')\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Could not remove file {file}: {e}\", 5, color='red')\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading LDOS data for alpha {alpha}: {e}\", 1, color='red')\n",
    "                continue\n",
    "        alphas_done.append(alpha)\n",
    "    \n",
    "    # remove unused alphas\n",
    "    data_out            = {k: v for k, v in data_out.items() if any(f'{a:.3f}' in k for a in alphas_done)}\n",
    "    data_out['alphas']  = alphas_done\n",
    "    # save the data to the directory\n",
    "    if len(data_out) > 0:\n",
    "        file_name = f\"ldos_data_ns={ns}_n={n}_{statetype}.h5\"\n",
    "        file_path = dir_save_parsed / file_name\n",
    "        HDF5Manager.save_data_to_file(directory=dir_save_parsed, filename=file_name, data_to_save=data_out, overwrite=True)\n",
    "    return data_out\n",
    "\n",
    "Nss     = [8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "Nss     = [8, 9]\n",
    "g0      = 1.0\n",
    "alphas  = np.arange(0.5, 0.98, 0.01)\n",
    "# alphas  = [0.760]\n",
    "# ops     = ['entanglement_entropy']\n",
    "ops     = ['Sz/0']\n",
    "state   = 'ME'\n",
    "MODEL   = 'um'\n",
    "n       = 1  # number of grains\n",
    "\n",
    "#! get the directories\n",
    "directory_data          = Directories(r\",ultrametric,Ns={},\" + f\"N={n}\" + \",g0=1.00,alpha={:.3f}\")\n",
    "directory_data_lips     = directory_lips_random_models  / directory_data\n",
    "directory_data_bem      = directory_bem2_random_models  / directory_data\n",
    "directory_data_lips2    = directory_lips_random_models2 / directory_data\n",
    "directory_data_bem2     = directory_bem2_random_models2 / directory_data\n",
    "dir_save_raw_new        = Directories('/media/klimak/ObiOne/FADING_RAN_MODELS/reparsed_all_together/time_evo', 'um')\n",
    "directory_data          = [directory_data_lips, directory_data_bem, directory_data_lips2, directory_data_bem2, dir_save_raw_new]\n",
    "directory_s             = Directories(os.curdir, 'saved', 'log', 'processed')\n",
    "\n",
    "for op in ops:\n",
    "    for Ns in Nss:  \n",
    "        logger.title(f\"Ns = {Ns}, operator = {op}\", 50, \"-\", 0)\n",
    "        opin    = Operators.resolve_operator(op, Ns)\n",
    "        get_ldos_full(\n",
    "            directories         = directory_data,\n",
    "            dir_save_parsed     = directory_s,\n",
    "            dir_save_raw_new    = dir_save_raw_new,\n",
    "            alphas              = alphas,\n",
    "            ns                  = Ns,\n",
    "            ops                 = [opin],\n",
    "            n                   = n,  # number of grains\n",
    "            statetype           = state,\n",
    "            model               = MODEL,\n",
    "            verbose             = True,\n",
    "            rm_and_mv_2_new     = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b26703",
   "metadata": {},
   "source": [
    "#### Get all "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e6d235",
   "metadata": {},
   "source": [
    "### Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fbc16f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas          = np.arange(0.68, 0.95, 0.06)\n",
    "alphas          = np.arange(0.3, 0.95, 0.02)\n",
    "# alphas          = [0.3, 0.74, 0.92]\n",
    "sites           = np.arange(7, 17, dtype=int)\n",
    "# sites           = np.arange(7, 12, dtype=int)\n",
    "n               = 1\n",
    "uniform         = True\n",
    "plotme          = len(sites) * len(alphas) < 10\n",
    "plotme          = False\n",
    "\n",
    "directory_s     = Directories(os.curdir, 'saved', 'uniform', 'processed')\n",
    "directory_s.mkdir(parents=True, exist_ok=True)\n",
    "for alpha in alphas:\n",
    "    for ns in sites:\n",
    "        directories_python      = [ Directories(os.path.abspath(data_dir), 'uniform' if uniform else 'log'),\n",
    "                                    Directories(os.path.abspath('data_final'), 'uniform' if uniform else 'log')]\n",
    "        try:\n",
    "            y, fig, ax              = get_results_single(directories_python =   directories_python,\n",
    "                                                        alpha               =   alpha,\n",
    "                                                        ns                  =   ns,\n",
    "                                                        n                   =   n,\n",
    "                                                        uniform             =   True,\n",
    "                                                        plotme              =   plotme)\n",
    "            if y is None:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing alpha {alpha:.2f}, ns {ns}, n {n}: {e}\", color='red')\n",
    "            continue\n",
    "        HDF5Manager.save_data_to_file(directory=directory_s, filename = f'alpha_{alpha:.2f}_ns_{ns}_n_{n}.h5', data_to_save=y)\n",
    "        if plotme:\n",
    "            plt.savefig(directory_s / f'alpha_{alpha:.2f}_ns_{ns}_n_{n}.png', dpi=150)\n",
    "            plt.savefig(directory_s / f'alpha_{alpha:.2f}_ns_{ns}_n_{n}.pdf', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "994b9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved  = Directories(\"/home/klimak/Codes/QuantumEigenSolver/Python/projects/2025/um_evolotion/saved/uniform/processed\")\n",
    "alphas      = np.arange(0.7, 0.8, 0.02)\n",
    "sites       = np.arange(7, 17, dtype=int)\n",
    "fig, ax     = Plotter.get_subplots(nrows=1, ncols=1, figsize=(6, 4), dpi=150)\n",
    "for alpha in alphas:\n",
    "    gap_ratios = []\n",
    "    nss_in     = []\n",
    "    colormap, color, cmap = Plotter.get_colormap(alphas, 'viridis_r')\n",
    "    for ns in sites:\n",
    "        try:\n",
    "            # data       = HDF5Manager.load_data_from_file(directory=path_saved, filename=f'alpha_{alpha:.2f}_ns_{ns}_n_1.h5')\n",
    "            data       = HDF5Manager.load_file_data(file_path = str(path_saved / f'alpha_{alpha:.2f}_ns_{ns}_n_1.h5'))\n",
    "\n",
    "            gap_ratio  = data.get('/stat/gap_ratio', None)\n",
    "            gap_ratios.append(gap_ratio)\n",
    "            nss_in.append(ns)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for alpha={alpha:.2f}, ns={ns}: {e}\")\n",
    "    print(f\"Alpha: {alpha:.2f}, Nss: {nss_in}, Gap Ratios: {gap_ratios}\")\n",
    "    if gap_ratios:\n",
    "        # gap_ratios = np.array([(np.abs(2 * np.log(2) - 1 - gr) if gr is not None else np.nan) for gr in gap_ratios])\n",
    "        gap_ratios = np.array([(np.abs(0.53 - gr) if gr is not None else np.nan) for gr in gap_ratios])\n",
    "        # gap_ratios = np.array([g if g is not None else np.nan for g in gap_ratios])\n",
    "        ax[0].plot(nss_in, gap_ratios, label=f'α={alpha:.2f}', marker='o', color=colormap(alpha), linestyle='-', linewidth=1.5, markersize=5)\n",
    "ax[0].set_xscale('linear')\n",
    "ax[0].set_xlabel(r'$\\mathcal{L}$')\n",
    "ax[0].set_ylabel(r'$\\bar r$')\n",
    "ax[0].set_yscale('log')\n",
    "Plotter.add_colorbar_pos(fig, color, mapable=alphas, pos=(0.95, 0.15, 0.02, 0.7), ylabel=r'$\\alpha$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeda150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
